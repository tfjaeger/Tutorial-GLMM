---
title: "Generalized Linear Mixed/Multilevel Models (GLMMs)"
subtitle: "An applied tutorial"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{tikz}
  - \usepackage{url}
  - \usetikzlibrary{bayesnet}
output:
  pdf_document: 
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  fontsize: 10pt
---

```{r set-options, include=F}
library(knitr)
opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=FALSE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 250),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
library(tidyverse)
library(magrittr)
library(broom)
```

```{r constants, include=F}
chains = 4

options(
  width = 1000,
  mc.cores = min(chains, parallel::detectCores()))
```

# TO DO

get trial level data

make visualization
read up on weibull

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{distribution $f$}} {} {}; %
    \node[det, above=of distribution] (mu) {$\mu_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{inverse link $g^{-1}$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome)} {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{Generalized linear model (GLM)}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_{i,j}$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$f$}} {} {}; %
    \node[det, above=of distribution] (mu) {$\mu_{i,j}$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$g^{-1}$}} {} {}; %
    \node[obs, above=of link] (X) {$x_{i,j}$} ; %
    \node[latent, right=of link] (betas) {$\beta_j$} ; %
    \factor[right=of betas] {group} {below:$\mathcal{N}$} {} {}; %
    \node[latent, above=of group] (Sigma) {$\Sigma$} ; %
    \node[latent, right=of group] (beta) {$\beta$} ; %
    \node[obs, right=of X, above=of betas] (grouplevel) {$z_j$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome)} {$N $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate2} {(mu) (distribution) (link) (betas) (X) (outcome) (grouplevel) (plate1) } {$M $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, betas, grouplevel} {link} ; %
    \edge {group} {betas} ; %
    \edge {Sigma, beta} {group} ; %
  }
  \caption{Generalized linear mixed model (GLMM)}
  \end{subfigure}
  \caption{The three components of the GLM: the linear predictor ($X\beta$), the link function $g$, and the distribution $f$. For the GLMM, $z_j$ is the $j$th level of the grouping variable $z$, which select which $\beta_j$ is to be chosen for the present case.}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$\mathcal{N}$}} {} {}; %
    \node[latent, right=of distribution] (sigma) {$\sigma$} ; %
    \node[det, above=of distribution] (mu) {$\mu_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$I^{-1}= I$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome)} {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu, sigma} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{Linear model}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Poisson$}} {} {}; %
    \node[det, above=of distribution] (mu) {$\lambda_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$log^{-1} = exp$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome)} {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{Poisson regression}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Bernoulli$}} {} {}; %
    \node[det, above=of distribution] (mu) {$p_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$logit^{-1}$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome)} {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{Logistic regression}
  \end{subfigure}
  \caption{Three different GLMs with their canonical link functions, and using typical notation (e.g., $\lambda$ instead of $\mu$ for the expected value of the Poisson model).}
\end{figure}





# Reading and assignments in *preparation* of this class

Please make sure you have read *all* of Gelman \& Hill (2007, p. 301-310 in Ch 14) as well as James et al. (2013, Ch. 4 up to but *not* including 4.3.4). To learn more about psychometric models, you might also find Gilchrist et al. (2005) helpful.

Then read and *work through* this document. We will use class to go through the important concepts and to address any questions that you have about the readings or the problem sets in this document. **Please note that this document is providing R code only.** Some of the steps described here might have less direct solutions in Matlab, so it is recommended that you start early. I have, however, tried to describe each step in a way that does not depend on R or Matlab.

In this document, you will find sections labeled "Prepare for class". Please work through those examples and write up your answers. If there are a few questions, you cannot answer, elicit help on the slack channel. In addition, there are sections called "Discussion questions for class". Please think about these questions, but don't worry if you get stuck. These are some questions we can go through during class.








# Quick recap: The linear model (LM)

A linear model (or linear regression) describes an outcome $y$ as the weighted sum of predictors $x_1$, ..., $x_k$, plus some error $\epsilon$ that is assumed to be normally distributed. This can be written in a variety of ways, e.g., for the specific $i$th of 1 to $n$ outcomes, $y_i$, or for the entire $n$-element vector of outcomes ${\mathbf y}$ with the elements $y_1$, ..., $y_n$, or in terms of the expected value of $E(\mathbf{y})$:

\begin{align}
y_i & = & \beta_0+\beta_1x_{1,i}+...+\beta_kx_{k,i} + \epsilon_i, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid}) & \Leftrightarrow \\
{\mathbf y} & = &  \beta_0+\beta_1{\mathbf x_1}+...+\beta_k{\mathbf x_k} + \epsilon, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid})  & \Leftrightarrow & \ \mathrm{[cf.\ James\ et\ al., 2013, p.\ 63]}\\
{\mathbf y} & = & X\beta + \epsilon, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid})  & \Leftrightarrow & \\
E({\mathbf y}) & = & X\beta &  & \Leftrightarrow & \\
\mathbf{y} & = & \mathrm{Normal}(X\beta, \sigma_{resid})&  &  & \ \mathrm{[cf.\ Gelman\ \&\ Hill, 2007, p.\ 38]}
\end{align}

where $X$ is a matrix with $k + 1$ columns and $n$ rows (the first column of which is a vector of $n$ 1s). $X$ is also sometimes called the *model (or design) matrix*.


## Assumptions of the LM

As a theoretical model the LM makes several assumptions (cf. Gelman \& Hill, 2007, p. 45). The first two assumptions are shared with many/most other statistical models we use (to be precise, some models don't strictly make these assumptions, but they do correct for their violations):

 + Independence of observations/errors
 + Validity/exhaustivity
 
The other assumptions are more specific to the LM, though the last two of these are shared with GLMs and GLMMs (but not GAMMs, for example):

 + Normality of errors
 + Equal variance of errors
 + Additivity of effects 
 + Linearity of each effects
 
Many, if not most, of these assumptions are actually violated when we apply the model. To some extent linear regression is robust to such violations, depending on the specific assumption. For example, instead of equality of variance it is often sufficient that variances are similar (homogeneity of variance) or that the variance-covariance matrix does not exhibit heteroscedasticity (from Ancient Greek hetero "different" and skedasis "dispersion", https://en.wikipedia.org/wiki/Heteroscedasticity; this extends the notion of homogeneity of variance to the *co*variance between variables). **But violations of the independence, normality, or homoscedasticity assumptions can make a model invalid, and the statistical inferences based on it invalid.**

## Using an LM (fit) for your analysis

The linear model is a theoretical model. If we employ it during data analysis, we are making the assumption that this model describes the relation between the predictors and the outcome in the population we seek to study. This also means that all of our conclusions are contingent on these assumptions, including in particular the assumption that we have considered all relevant variables (or have otherwise ruled out that other effects can confound our analysis).

The coefficients of the LM (the $\beta$s) are parameters of this theoretical model (the coefficients), and we do not know their true value. Neither do we know the true values of the outcome, but we assume that our observations of the outcome variable are (potentially noisy) independent samples drawn from the outcome. Additionally, we often don't really know the predictors we believe to have causal effects on the outcome, but rather we have observable *measures* of these predictors. The linear model does *not* take this into account (but there are models that extend the LM that do).

When we use an LM to analyze our data, we fit a *specific* LM---i.e., an LM with a specific set of predictors to our data (the specific outcome). We specify this model using regression formula syntax. For example in R, the following would describe the formula for an LM that regresses an outcome variable Threshold against the intercept (1) and a predictor called Condition:

$$ Threshold \sim 1 + Condition $$
The algorithm implemented in whatever function we use for that (e.g., *fitlm* in Matlab or *lm* in R) then determines the best-fitting estimates of the *coefficients*. In other words, we tell the LM-fitting function the outcome and predictor---the knowns---and the function then determines the coefficients---the unknowns. The resulting combination of the specific LM (the predictors and outcome variables) and the best-fitting coefficient estimates together constitute the **fit** or **fitted (linear) model**. The statistical inferences we draw, and write-up, are always based on such best-fitting models. Here we won't go into detail about how these "best-fitting" estimates and their standard errors are obtained, but we will return to that later in the semester.

## Writing up results

We should **use $\widehat{\beta}$ when we describe coefficient estimates and write up results**. This emphasizes that these are *estimates* of the assumed population parameters ($\beta$). It is worth noting that the $\widehat{\beta}$s are not the only outputs we obtain from a LM fit. This fit also contains, for example, predicted values for each outcome based on the predictors for that observation. But the null hypothesis significance testing (NHST) that we conduct focuses on the best-fitting coefficient estimates and their standard errors. When we write that a predictor did or did not have a statistically significant effect, or when we talk about the size of the effect, those statements refer to the coefficient estimate, its standard error, and measures derived from them.


# The data for this document

For this tutorial we are continuing to use Ashley Clark's data from her experiment on visual crowding effects on foveal processing. Here's a copy of her description of the data. One difference to the data from her study that you've seen so far is that we're including a third condition that Clark and colleagues collected as a control.

## Background
Crowding is a visual phenomenon that has puzzled scientists for decades; an object in isolation can be perceived without problems, yet surrounding it with similar objects makes it harder to see. While crowding has been studied extensively in the visual periphery, humans normally orient objects in their center of gaze, the high-acuity region of the retina called the foveola. While the foveola is less than 1.5mm wide (or ~1 visual deg2), it contains more cones than rest of the retina combined. Human’s ability to actively perceive the visual world relies heavily on not only the foveola itself, but also how and where the eye is positioned. The few studies that have examined crowding within foveal vision have produced contradictory results. Some reasons for these discrepancies include using relatively large stimuli, a small number of participants, abnormal stimuli presentation, and having indefinite stimulus presentation times with no eye tracking. More recent research, however, has highlighted the importance of precise eye tracking due to the eye’s constant movement during even fixation. These small and constant movements of the eye, called fixational eye movements (FEMs), are beneficial or both high acuity vision, as well as daily tasks such as reading and face recognition. FEM’s have never been examined in the context of crowding, and it remains unknown how individuals in previous crowding studies directed their foveola over stimuli, or even maintained fixation.

## Goals
The goals of this research are to (1) investigate the effect of crowding within the foveola, and (2) examine if and how fixational eye movements influence crowding at this scale. Based on previous research, we hypothesize that crowding will be detrimental to foveal vision, as it is in peripheral vision, but on a finer scale. Further, based on the recent findings that FEMs are beneficial for high-acuity vision, I expect a relationship between FEMs and the strength of crowding within the foveola, with larger and less precise FEMs increasing the negative effects visual crowding.

## Methods
Studying FEMs during visual crowding within the foveola requires high-precision eye tracking and accuracy in localizing the center of gaze. Current video eye trackers do not have the required spatial precision, as the error of gaze localization is as large as the foveola itself. However, by using a custom built state-of-the-art eye tracking system with arcminute precision, we will be able to examine exactly how FEMs contribute to crowding within the foveola. Stimuli consist of a number-font designed specifically for studying crowding in the fovea, as it allows for recognition even when numbers are closer together than traditional optotypes used in other crowding studies.6 Two conditions will be examined, the uncrowded (where a single number is presented), and the crowded (where the same size number is presented, but with four surrounding numbers). The size of the number and spacing between the numbers in the crowded condition change throughout the experiment based on the subject’s performance using an adaptive procedure. The stimuli presented will vary in size, ranging from 0.5 arcminutes to 4 arcminutes in width. To determine the number-width threshold, we use a standard psychophysics procedure measuring the width of the stimulus at which a subject performs above chance level. New variables at a trial level include the followding

 * Size: the width (or strokwidth) of the stimulus during that group of trials.
 * Performance: What the overall performance for the size stimulus was.
 * Response: What number the subject guessed.
 * Answer: What the number presented on the screen was.
 * Correct: Wether the response was correct or not (combining information from Response and Answer)
 * Response Time: response time of subject. Note* subjects were not told to response as quickly as possible during experiment.
 * Traces: the x and y traces of each trial in this size stimulus group.
 * Trial Curvature: Same as curvature above, but for each individual trial.
 * Trial Speed: Same as speed above, but for each individual trial. 

## Overview
```{r load data, message=F}
d = read_csv("../data/data_ClarkCrowding_TrialLevel.csv") %>%
  mutate_at(c("Subject", "Condition"), factor)
d %<>%
  filter(Condition != "Fixation") %>%
  na.omit() %>%
  droplevels()
d
```






### Prepare for class 
 
 1. On Figure 3, draw line segment that correspond to the intercept. 
 1. On Figure 3, draw line segment that correspond to the effect of condition.
 1. Calculate the slope for the crowded condition from the model output shown above.
 1. What's a geometric interpretation of an interaction between two *continuous* predictors (x1 and x2)? For this it might be helpful to first think about the geometric interpretation of two additive continuous predictors (a plane over x1 and x2, the height of which is given along the third axis, y)
 1. True or false? Since the interaction does not have a significant effect, we can safely remove it from the model. 

### Discussion questions for class

 6. True or false? Now that we know that the slope of diffusion constant does not significantly differ between the two crowdedness conditions, and given that the effect of diffusion constant was significant, we can conclude that both conditions exhibit a significant effect of the diffusion constant. 
 1. True or false? If we had found a significant interaction between condition and diffusion constant, we could have concluded that the effect of conclusion constant goes in opposite directions for the two conditions? 
 1. When we fit the same interaction model with the uncentered diffusion constant instead, we get a seemingly conflicting result, where condition does not longer have a significant effect. Why is that the case? Is this result really conflicting? (The figure might help. Hint: draw the line segments corresponding to the effect of condition in this new interaction model.)
 1. Are the predicted/fitted values from this model different from the first interaction model we fit above?

## Writing up the analysis results

\color{lightgray}
We analyzed the data with a linear regression, using the function \texttt{lm} from the \texttt{base} package (citation with version) of the software \texttt{R} (citation with version). We calculated each subject's mean thresholds for the crowded and uncrowded condition. These 20 threshold values were regressed against condition (deviation-coded: .5 = *crowded* vs. -.5 = *uncrowded*), the diffusion constant **(centered), and their interaction**. We found a statistically significant **main** effect of condition ($\widehat{\beta}=.442, t=4.697, p<.01$), so that subjects reach threshold performance at larger sizes in the crowded condition, compared to the uncrowded condition. We also found a statistically significant effect of the diffusion constant ($\widehat{\beta}=.037, t=5.846, p<.01$), so that larger diffusion constants required larger sizes to achieve threshold performance. **The interaction was not significant ($\widehat{\beta} = 0.009, p > .5$).**
\color{black}






# Weibull regression

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Weibull$}} {} {}; %
    \node[det, right=of distribution] (shape) {$k$} ; %
    \node[det, above=of distribution] (mu) {$\lambda_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$log^{-1} = exp$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome) } {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu, shape} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{only scale parameter ($\lambda$) is inferred}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Weibull$}} {} {}; %
    \node[latent, right=of distribution] (shape) {$k$} ; %
    \node[det, above=of distribution] (mu) {$\lambda_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$log^{-1} = exp$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome) } {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu, shape} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{both shape ($k$ and scale parameters ($\lambda$) are inferred}
  \end{subfigure}
  \caption{The Weibull regression is {\em not} a GLM, unless the shape parameter $k$ is fixed (a). The Weibull model with both its scale and shape parameter can thus be seen as an infinite set of GLMs.}
\end{figure}




# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
