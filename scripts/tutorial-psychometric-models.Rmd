---
title: "Psychometric models"
subtitle: "A brief applied overview"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{tikz}
  - \usepackage{url}
  - \usetikzlibrary{bayesnet}
  - \usepackage{animate}
output:
  pdf_document: 
    extra_dependencies: animate
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
always_allow_html: true
urlcolor: blue
---

```{r set-options, include=F}
library(knitr)
opts_chunk$set(
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=TRUE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 110),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
library(tidyverse)    # gotta be tidy
library(magrittr)     # pipes!
library(broom)        # working with model output
library(broom.mixed)  # working with mixed model output
library(brms)         # Bayesian GLMMs and NLMMs
library(tidybayes)    # working with posterior samples of Bayesian models

library(ggthemes)  # cool themes!
library(scales)    # to plot predictions on original scale of predictor
library(gganimate) # animations
library(gifski)
library(plotly)    # 3D interactive plots
```

```{r constants, include=F}
chains = 4
options(
  width = 110,
  mc.cores = min(chains, parallel::detectCores()))
```

```{r functions, include=F}
my_hypotheses = function(model, prefix= "eta", link_lambda = "identity") {
  if (link_lambda == "logit") 
    cat("NB: converting estimate and CIs for lambda into proportion space.\n The SE of the estimate, however, is in logits (since it cannot meaningfully be expressed in proportion space).")
  
  lambda_h <- .05
  
  hypothesis(model, paste0(prefix, "_Size > 0"))$hypothesis %>%
    mutate(Hypothesis = c("$\\beta_{Size}$ > 0")) %>%
    { if (paste0("b_", prefix, "_ConditionCrowded.vs.Uncrowded") %in% variables(model)) 
      rbind(
        ., 
        rbind(    
          hypothesis(model, paste0(prefix, "_ConditionCrowded.vs.Uncrowded < 0"))$hypothesis,
          hypothesis(model, paste0(prefix, "_Size:ConditionCrowded.vs.Uncrowded < 0"))$hypothesis) %>%
          mutate(Hypothesis = c("$\\beta_{Crowded}$ < 0", "$\\beta_{Size:Crowded}$ < 0"))) else . } %>%
    { if (any(c("b_lambda_Intercept", "b_lapse_Intercept") %in% variables(model))) 
      rbind(
        ., 
        hypothesis(
          model, 
          paste0(
            intersect(
              c("b_lambda_Intercept", "b_lapse_Intercept"), 
              variables(model)), " < ", if (link_lambda == "logit") qlogis(lambda_h) else lambda_h), 
          class = NULL)$hypothesis %>%
          { if (link_lambda == "logit") 
            mutate_at(
              .,
              vars(Estimate, CI.Lower, CI.Upper),
              plogis) else . } %>%
          mutate(Hypothesis = paste("$\\lambda <$", lambda_h))) else . } %>% 
    mutate_if(is.numeric, ~ round(.x, digits = 3)) %>%
    kable()
}

my_hypotheses_mixture = function(model) my_hypotheses(model, prefix = "mu2")
```

# Goals of this tutorial
This tutorial aims to illustrate the relation between psychometric models (as used in psychophysics) and generalized linear models (GLMs) / generalized linear models (GLMMs). It's best to read it *after* reading the accompanying GL(M)M tutorial within the same git repository. 

Psychometric models aim infer the 'threshold' and 'slope' of a categorization/recognition function along one or more stimulus dimensions (e.g., visual contrast, intensity, size, etc.). One challenge in doing so is that the exact function is unknown, though plausible candidates---such as the Weibull, Gumbel, logistic, cumulative Gaussian, etc.---exist ([quick side-by-side of these functions](http://psignifit.sourceforge.net/PSYCHOMETRICFUNCTIONS.html)). Another challenge is that the estimates of the threshold and slope parameters can be affected (and biased) if attentional lapses are not considered or fixed to specific values. These biases persist even if the true lapse rate is constant across conditions of interest.

\textbf{This tutorial is very much a work in progress and will benefit from your questions and comments.} I am aware that many sections are lacking context and some sections are incomplete. It is also possible that parts of the tutorial still contain errors. So please ask / comment away.


# Readings and other materials

To learn more about psychometric models, you might also find Gilchrist et al. (2005) and Wichmann and Hill (2001a, b) helpful (both in *Perception & Psychophysics*). Prins (2012, 2013, *Journal of Vision*) provide a counterpoint to the solution proposed in Wichmann and Hill (2001). All of these papers are relatively heavy on math but the Prins readings are comparatively accessible. 

Luckily, there is a wealth of information out there, including introductory walk-throughs and powerful animated demonstrations. Here are some walk-through demonstrations (thanks to Martina Poletti for pointing me to these resources):

 * \url{http://www.dlinares.org/lapsesquickpsy.html} (partially replicating Wichmann’s point about biases, but also replicating Prins’s failure to replicate Wichmann’s proposed solution)
 * \url{http://www.palamedestoolbox.org/understandingfitting.html} (beautiful animations and visualizations explaining the fitting process, and the relation between lapse, threshold, and slope; really drives home the point that these three parameters form a joint distribution for which we’re trying to find the set of values that maximize the likelihood of the observed outcomes). Also goes through the Prins and Palamedes papers, and ends with a list of tips.
 * \url{http://www.palamedestoolbox.org/weibullandfriends.html} (Nice explanation of the psychometric-specific jargon and naming of models; e.g., Gumbel vs. Weibull, a matter of log-transforming the predictor)
 
Some toolboxes/libraries for psychometric models:

 *  Matlab: 
   * [psignifit/](https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/neuronale-informationsverarbeitung/research/software/psignifit/)
 * R: 
   * [quickpsy](http://dlinares.org/quickpsy.html)
   * [psyphy](https://cran.r-project.org/web/packages/psyphy/index.html) include GLM-based psychometric model fitting, lapse models, and a variety of psychometric models (incl. cumulative Gaussian)
   * [modelfree](https://personalpages.manchester.ac.uk/staff/d.h.foster/software-modelfree/latest/home): non-parametric estimation of psychometric functions.
   * [brms](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf) can in principle fit outcomes that follow a Bernoulli, Weibull, or Generalized Extreme Value Distribution. This should cover the logistic, Weibull, Gumbel, and Generalized Extreme Value models for psychometric fitting. It's not yet clear to me whether \texttt{brms} would also be able to fit the cumulative Gaussian model.The main appeal of \texttt{brms} in my view is that it's part of a more general framework of Bayesian model estimation and thus access to the full posterior distribution of all parameters (e.g., via \texttt{tidybayes}). [Summary of distributional families that can be specified in \texttt{brms}](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html).
   
Further readings:

 * Abrahamyan et al. (2016) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4922170/ (on adaptive procedures)
 * Jigo & Carrasco (2020) https://jov.arvojournals.org/article.aspx?articleid=2770148 (on exogenous and endogenous attention and their effects on the psychometric function)




# Psychometric models

Psychometric models consist of two parts: a perceptual model that describes behavior (or neural responses) as a function of the stimulus and a component that describes what happens on the trials when a subject does not respond based on the stimulus. 

## Perceptual model

In psychometric modeling, we predict subjects' responses from a stimulus value. Let's refer to this as the *perceptual model*. A number of different types of perceptual models are common in the field, with choices varying between labs, based on goodness of fit, or field-specific conventions. All of these models fall into the class of non-linear models (NLM) and some are also generalized linear models (GLMs). The goal of this section is to illustrate the relation between GL(M)Ms and psychometric functions a bit further, so that your understanding of GL(M)Ms can also inform your analysis choices for psychometric data.

Four commonly used perceptual models include the cumulative Gaussian, logistic, Weibull, and Gumble (Generalized Extreme Value distribution a.k.a. log Weibull). For formal introductions, the relation between the different functions, and their relative trade-offs, see the readings listed at the top of this tutorial. Theoretically, psychometric models can accommodate nonlinear effects of the stimulus as well as effects of multiple cue dimensions. Most commonly, however, the psychometric model is presented as a function of two parameters, $\alpha$ and $\beta$, that jointly describe a linear relation in a space determined by the type of perceptual model. For example, the standard logistic perceptual model describes a linear effect of the stimulus on the log-odds of a correct response:

\begin{align}
\log \frac{p(correct\ response | stimulus)}{p(incorrect\ response | stimulus)} = \alpha + \beta * stimulus
\end{align}


## Lapsing model (Wiechmann and Hill formulation)

Any perceptual model can be expanded to account for the proportion of trials on which the subject responds *in*dependent of the stimulus (e.g., because the subject experienced an attentional lapse). This is done by adding a parameter that accounts for the rate of such lapsing trials, and optionally for the response bias (or guessing rate) on those lapsing trials. This lapse model can specified in two different ways. First, in the Wiechmann and Hill (2001) formulation:

\begin{align}
p(correct\ response | stimulus, \alpha, \beta, \lambda, \gamma) = \gamma + (1 - \lambda - \gamma) * p(correct\ response | stimulus, \alpha, \beta) 
\end{align}

where $p(correct\ response | stimulus, \alpha, \beta)$ is the \textbf{perceptual model}---i.e., the function that maps input stimuli onto responses (e.g,. the logistic model). This perceptual model depends on two parameters $\alpha$ and $\beta$ (under the the general modeling framework used in this tutorial, we can include further effects, including non-linear effects of the stimulus). The specific interpretation of these two parameters depends on the psychometric function, but each of the functions can be parameterized such that $\alpha$ is some targeted threshold performance, and $\beta$ is the 'slope' at that threshold. In addition to the two parameters of the perceptual model, the formula contains two more parameters: $\gamma$, which is sometimes called the {\em guess} rate, and $\lambda$, which is sometimes called the {\em lapse} rate. Guess and lapse are seemingly intuitive but ambiguous terms, which can cause confusion. In the Wiechmann and Hill formulation, $\gamma$ describes the performance that would be achieved if the participants guessed on all trials (i.e., chance performance), rather than the proportion of trials on which the participant guesses or the bias in the participant's responses *when* the participant guesses. Under this formulation, \textbf{$\gamma$ thus describes floor performance: the minimum proportion of correct answers}. The second parameter in the Wiechman and Hill formulation, $\lambda$, describes the proportion of trials on which participants provide the wrong answer, rather than the proportion of trials on which the participant does not respond based on the stimulus (e.g., because of attentional lapses). Under the Wiechman and Hill formulation, \textbf{$1 - \lambda$ thus describes the ceiling performance: the maximum proportion of correct answer}. This is the formulation that I use for most of the analyses below.

## Lapsing model (mixture formulation)

An alternative formulation of the same process, describes the lapse model as a mixture model of two components. This formulation is computationally more efficient and easier to fit, as it orthogonalizes the two parameters that jointly determine the lapsing model ($\gamma$ and $\lambda$ in the Wiechmann and Hill formulation). But as far as I can see, the mixture formulation does not allow holding $\gamma$ or $\lambda$ constant. That is, the mixture formulation is efficient for scenarios in which we want to estimate both $\gamma$ or $\lambda$ (see example in appendix).

Like in the Wiechmann and Hill formulation, one component of the mixture formulation is the perceptual model describing how responses depend on the stimulus for those trials on which participant does respond based on the stimulus. The second component of the mixture model describes responses for trials on which the participant does not respond based on the stimulus. On those trials, the participant is assumed to respond with some *bias* that is independent of (uninformed by) the stimulus. Finally, the respective weights of the two components is determined by the proportion of trials on which the participant does not respond based on the stimulus (*lapse*). Note that *lapse* here is not the same as $\lambda$ under the Wiechmann and Hill formulation (neither is *bias* the same as $\gamma$/*guess* ). 

\begin{align}
p(response | stimulus, \alpha, \beta, lapse, bias) = (1-lapse) * p(response | stimulus, \alpha, \beta) + lapse * bias
\end{align}

Under this second formulation, the floor performance is $lapse * bias$ and the ceiling performance is $1 - (lapse - lapse * bias) = 1 - (1 - bias) * lapse$. While the *lapse* and *bias* parameters of this formulation are not the same as the $\lambda$ and $\gamma$ parameters of the Wiechmann and Hill formulation, the parameters can be translated into each other: 

\begin{align}
lapse = \gamma + \lambda \\
bias = \frac{\gamma}{(\gamma + \lambda)} \\
\lambda = (1 - bias) * lapse \\
\gamma = lapse * bias
\end{align}

For a demonstration of the second formulation, you can visit the interactive website at \url{https://hlplab.shinyapps.io/BayesianIdentificationAndDiscrimination/}. The site's first tab describes category identification (categorization) and perceptual discrimination for two Gaussian categories by an ideal observer. We'll focus on the "Ideal identification" plot (right side, in the middle of the lower half of the screen) and how it changes if you changes as a function of the rate of attentional lapsing (top left) and bias (unclick "Prior as bias" and then try out different biases). The ideal categorization function between two Gaussian categories turns out to be a logistic function (with linear and quadratic stimulus effects)---i.e., one of the psychometric functions mentioned above (and a GLM). So the effects of lapse rate and bias that you see on this site give you an initial idea of how these parameters can affect the fit of psychometric functions.

The two formulations describe the same outcomes but split up the relevant quantities in different ways. Regardless of the formulation, the lapse model is *not* a GLM anymore. 







# A simple example

This tutorial uses the R library \texttt{brms} for Bayesian regression modeling, which---among many other things---fits non-linear models. \texttt{brms} converts GL(M)Ms, GA(M)Ms, NL(M)Ms, and a number of other model types into \texttt{stan} code. \texttt{stan} is a model specification language with interfaces to Matlab, R, Python and other languages. It comes with an efficient sampler. \texttt{stan} programs are compiled into C++ code, sampled from, and the resulting posterior samples of the fitted model are returned ... in this case to R's \texttt{brms} functions. Using a Bayesian approach has a number of upsides, one of which is that the straightforwardness of obtaining (Bayesian) confidence intervals for any of the parameters in the model.

## Data
We use the same data as in the GLMM tutorial from Ashley Clark's (as of 2022 unpublished) experiment on foveal crowding. \textbf{Do not share this data without Ashley's permission. Thank you.} The goals of Ashley's research are to (1) investigate the effect of crowding within the foveola, and (2) examine if and how fixational eye movements (FEMs) influence crowding at this scale. Based on previous research, we hypothesize that crowding will be detrimental to foveal vision, as it is in peripheral vision, but on a finer scale. Further, based on the recent findings that FEMs are beneficial for high-acuity vision, I expect a relationship between FEMs and the strength of crowding within the foveola, with larger and less precise FEMs increasing the negative effects visual crowding.

Here, your goal is to test whether crowding---operationalized through two crowding conditions described below---affects the accuracy of number recognition. You will assess this while controlling for the size of the letters/numbers on the screen, as stimulus size is known to have large effects on recognition accuracy. We test two working hypotheses:

 * (H1) Crowding will reduce recognition accuracy (after controlling for letter size)
 * (H2) Letter size will have an increased effect in the more crowded displays.

### Methods
Studying FEMs during visual crowding within the foveola requires high-precision eye tracking and accuracy in localizing the center of gaze. Current video eye trackers do not have the required spatial precision, as the error of gaze localization is as large as the foveola itself. However, by using a custom built state-of-the-art eye tracking system with arcminute precision, we will be able to examine exactly how FEMs contribute to crowding within the foveola. Stimuli consist of a number-font designed specifically for studying crowding in the fovea, as it allows for recognition even when numbers are closer together than traditional optotypes used in other crowding studies.6 Two conditions will be examined, the uncrowded (where a single number is presented), and the crowded (where the same size number is presented, but with four surrounding numbers). The size of the number and spacing between the numbers in the crowded condition change throughout the experiment based on the subject’s performance using an adaptive procedure. The stimuli presented will vary in size, ranging from 0.5 arcminutes to 4 arcminutes in width. To determine the number-width threshold, we use a standard psychophysics procedure measuring the width of the stimulus at which a subject performs above chance level. 

### Data overview

```{r load data, message=F}
d = read_csv("../data/data_ClarkCrowding_TrialLevel.csv") 
d %<>%
  na.omit() %>%
  droplevels() %>%
  rename(
    Condition = Crowded,
    Threshold.Subj = Threshold,
    DiffusionConstant.Subj = DiffusionConstant,
    Span.Subj = Span,
    Area.Subj = Area,
    Curvature.Subj = Curvature,
    Speed.Subj = Speed,
    Size = Size,
    Size.AvgPerformance = Performance,
    ResponseExpected = Answer,
    ResponseCorrect = Correct,
    Curvature = TrialCurvature,
    Speed = TrialSpeed,
    Span = TrialSpan
    ) %>%
  mutate(Condition = factor(ifelse(Condition == 0, "uncrowded", "crowded"), 
                            levels = c("uncrowded", "crowded"))) %>%
  select(Subject, Condition, Threshold.Subj, DiffusionConstant.Subj, Area.Subj, Span.Subj, Speed.Subj, everything(), Span, Speed, Curvature) %>%
  mutate_at(c("Subject"), factor)

str(d)
```


## Setting up the model

As a first illustration, here is the formula describing a psychometric model with a logistic link function for the case of non-hierarchical data (data from a single subject). \texttt{brms} also allows us a more elegant mixture formulation (see appendix and replies to \url{https://discourse.mc-stan.org/t/fitting-lapsing-psychometric-functions-with-brms/5762/2}) but the non-linear model implementation following Wiechmann and Hill (2001) is more transparent for the present purpose:

```{r, echo=T}
# Defining a brms formula for the non-linear model. It has three components:
# the lapse rate, the guess (bias), and the linear predictor eta. Each of these
# components can be described by a linear predictor. For this psychometric 
# model we describing eta as the linear combination of the intercept and the 
# effect of the stimulus. The other two parameters are just described by only
# their respective 'intercepts'.
BF <- brmsformula(
  ResponseCorrect ~ gamma + (1-gamma-lambda) * inv_logit(eta),
  eta ~ 1 + Size,
  gamma ~ 1,
  lambda ~ 1, 
  family = bernoulli(link="identity"),
  nl = TRUE
) 
```

Typically, the design of psychometric experiments is set up such that chance level accuracy ($\gamma$) is known. In that case, the model simplifies somewhat. For example, for the 4AFC task employed in Ashley's data, we would get chance level performance of 25% and thus $\gamma = .25$. This simplifies the model to the following formula:

```{r, echo=T}
BF.lambda <- brmsformula(
  ResponseCorrect ~ .25 + (1-.25-lambda) * inv_logit(eta),
  eta ~ 1 + Size,
  lambda ~ 1, 
  family = bernoulli(link="identity"),
  nl = TRUE
) 
```

For non-linear models, it can be important to incorporate constraints on the parameters, or any knowledge we have about plausible values for the parameters. Since we are taking a Bayesian approach here, this is achieved through the definition of priors. Throughout this tutorial, we only use very weakly regularizing priors---i.e., priors that do not bias the coefficient values in either direction but that 'pull' them closer to zero (following recommendations from \url{https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations}). In effect, these priors implement Occam's razor: unless there is sufficient evidence in the data itself, we assume that the null hypothesis is true for all of our parameters. We can also define lower and upper bounds (lb and ub in the code) for, e.g., the $\lambda$ parameter. For example, for a model with a lower bound of 0 and an upper bound of .5 for $\lambda$:

```{r, echo=T}
# Define weakly regularizing prior, including lower (lb) and upper bounds (ub)
# for lambda
my.priors.lapse <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"),
  prior(beta(1, 1), nlpar = "lapse", lb = 0, ub = .5)
)
```

## Preparing the data

So that the priors are on the right 'scale', we follow (Gelman et al., 2008), and standardize the continuous predictors in the model. As is typical for analyses of experimental data with two balanced conditions, we deviation/sum/abova/effect-code the condition variable, with the 'treatment' receiving the positive predictor value:

```{r, echo=T}
# We store the mean and sd of Size so that we can later transform the model's predictions back 
# onto the scale of original Size predictor
Size.mu = mean(d$Size)
Size.sd = sd(d$Size)

# Standardize Size and make sure order of levels for Condition is as intended
d %<>%
    mutate(
      Condition = factor(Condition, levels = c("uncrowded", "crowded")),
      Size = (Size - mean(Size)) / (2 * sd(Size)))

# Sum-code condition
contrasts(d$Condition) = cbind("Crowded.vs.Uncrowded" = c(-.5,.5))
```

## Fitting the model

We fit the model to the data from Subject 1 with the function \texttt{brm()} from the library \texttt{brms}, and summarize the output:

```{r simple-example-results, message=F, warning=T, error=F}
fit.lambda0.1 <- brm(
  BF.lambda,
  data = d %>%
    filter(Subject == "1"),
    control = list(adapt_delta = 0.999),
  iter = 8000,
  thin = 5,
  prior = my.priors.lapse,
  file = "../models/subject1-lambda-inferred-constrained"
)

summary(fit.lambda0.1)
```

We can test the hypotheses we hold about the various parameters in the model using Bayesian hypothesis testing, implemented via the function \texttt{hypothesis} foorm the same package:

```{r simple-example-results-hypothesis-testing}
my_hypotheses(fit.lambda0.1)
```

And, finally, we visualize the fit against the data, the predictions of the model along with 95\% uncertainty intervals.

```{r simple-example-plots}
p = plot(conditional_effects(fit.lambda0.1), 
         plot = F,
         points = T, 
         point_args = list(alpha = .05, height = .025, width = .025),
         theme = theme_fivethirtyeight())[[1]]

# Some common plot components to be reused below
unstandardize = function(x) { x * Size.sd + Size.mu }
standardize = function(x) { (x - Size.mu) / Size.sd }

p.common =  
  list(geom_hline(yintercept = .25, color = "black", linetype = 2),
       scale_x_continuous(
         "Letter size"), 
       scale_y_continuous(
         "Probability of correct response"),
       coord_trans(
         x = trans_new(
           "unstandardize",
           transform = "unstandardize",
           inverse = "standardize")
       ))
p + p.common
```

As this illustrates, fitting a lapsing model (likely adequately) leads to substantial uncertainty about $\lambda$, $\alpha$, and $\beta$. This is not a rare scenario: for psychometric data with only a few measurement points along the mid-performance range of the stimulus, we can often not distinguish between effects of attentional lapses and the effect of the stimulus. This is worth noting since higher lapse rates in a model like this one will result in steeper (larger) slope estimates, and will also affect our threshold estimate. We illustrate this in the next section, right after we have shown how to obtain the threshold estimate from a model like the one we have just fit.

## Determining the subject-specific threshold

Psychometric models are often used to obtain thresholds and slopes that are then submitted to secondary analysis. For example, we might obtain a stimulus threshold---i.e., a stimulus value at which a certain threshold performance (accuracy) is achieved---and then analyze whether that stimulus threshold differs across conditions. From a model like that fit in the previous section, we can obtain both naive and lapse-corrected performance thresholds. For lapse-corrected threshold, we need to solve the logistic part of the model for the desired threshold. Note that this is an approximate estimate. To get a better estimate, we should marginalize over all posterior samples. This will not necessarily the same as using the (mean) estimates for both the intercept and slope since the posterior distributions of the intercept and slope might be correlated. 

\begin{align}
logit(threshold) & = & \alpha + \beta * Size 
\end{align}

For example, for a threshold of .625, we set:

\begin{align}
logit(.625) & = & \alpha + \beta * Size & \Rightarrow \\
0.5108256 & = & 1.04 + 2.00 * Size & \Leftrightarrow \\
-0.2645872 & = & Size
\end{align}

```{r, warning=F}
p + 
  p.common +
  geom_segment(x = -0.2645872, xend = -0.2645872, y = 0, yend = .625, color = "red", inherit.aes = F)
```

Similarly, we can obtain the lapse-corrected slope from the model. It is simply $\beta$. Being able to obtain thresholds or slopes is useful given the current standards of the field. Note though that off-the-shelf analyses techniques like GLMMs and extensions thereof make allow for a more direct approach of testing the effect of other factors (such as conditions) by simply including those predictors in the psychometric model. This more direct approach has the advantage that it does not throw away trial-level data (as the standard secondary analyses do not have access to the uncertainty of the threshold and slope estimates obtained from the psychometric model), allowing the combined model to more appropriately account for sources of uncertainty. The analyses presented in the remaining sections contain examples of this approach. 

The advantage of this more direct approach becomes even further pronounced when we combined the data from all subjects and submit it to trial-level mixed-effects (hierarchical/multilevel) analyses. These analyses allow us to test whether the effects we observe in the trial-level data (e.g., about the effect of condition on aspects of the perceptual model) generalize across the population of interest. 

# Illustrating the consequences of assumptions about the lapse rate ($\lambda$)

This section first present a simulation study (for which the ground truth is known) and then analyses based on Ashley Clark's data (for which the ground truth is unknown). Both case studies are intended to illustrate the consequences of assumptions about lapse rates---i.e., what happens when lapse rates are assumed to have a certain value, rather than being estimated.

## Simulation study

For our simulation study, we continue to consider a simple model as in the previous section, with only the intercept and an effect of the stimulus (rather than considering additional conditions). We first make a data generator for a 2AFC task that creates data following the Wichmann and Hill (2001) model with a logistic response function:

```{r simulation-generate-data-generator, echo=T}
gelman_scale = function(x) {
  x = (x - mean(x)) / (2 * sd(x))
}

generate_data = function(
  intercept = 1,
  slope = .5,
  lambda = 0, 
  gamma = .5,
  stimuli = seq(-3, 3, length.out = 20),
  n_per_stimulus = 20
) {
  crossing(
    stimulus = stimuli,
    trial = 1:n_per_stimulus) %>%
    mutate(
      s_stimulus = gelman_scale(stimulus),
      p = inv_logit_scaled(
        intercept + slope * stimulus,
        lb = gamma,
        ub = 1 - lambda),
      accuracy = rbinom(nrow(.), 1, p))
}

# generate data
true.intercept = -4
true.slope = 3
true.gamma = .5
true.lambda = .05
d.temp = generate_data(
  intercept = true.intercept,
  slope = true.slope,
  gamma = true.gamma,
  lambda = true.lambda, 
  stimuli = seq(0, 3, length.out = 10), 
  n_per_stimulus = 100) 
```

```{r simulation-define-analyses, include=T}
par.n_choices = 2
par.lambda = c(0, .025, .05, .1)
par.gamma = 1 / par.n_choices

my.priors <- c(prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"))
analyses = list()
for (i in 1:length(par.lambda)) {
  for (j in 1:length(par.gamma)) {
    f = paste("accuracy ~", par.gamma[j], "+", 1-par.gamma[j]-par.lambda[i], "* inv_logit(eta)")
    
    analyses[[paste(par.lambda[i], par.gamma[j])]] <- brm(
      brmsformula(
        f,
        # Change to use s_stimulus if the gelman scaled stimulus is desired.
        # This will mean the intercept is the estimate at the center of the data.
        eta ~ 1 + stimulus,
        family = bernoulli(link="identity"),
        nl = TRUE),
      data = d.temp,
      chains = 0,
      prior = my.priors,
      file = paste0("../models/sim-model-gamma", par.gamma[j], "-lapse", par.lambda[i]))
  }
}
```

Here is an example draw from the data generator with 100 data points per stimulus at five different stimulus locations between 0 and 3 for $\lambda = .05$. The plot shows a fit if the lapse rate is assumed to be zero:

```{r simulation-example-data-plots, warning=FALSE}
# without lapse
d.temp %>%
  ggplot(aes(x = stimulus, y = accuracy)) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  geom_hline(yintercept = true.gamma, linetype = 2, color = "darkgray") +
  geom_line(
    data = tibble(stimulus = seq(0, 3, length.out = 100), group = 1),
    aes(
      y = inv_logit_scaled(
        true.intercept + true.slope * stimulus, 
        lb = true.gamma, 
        ub = 1),
      group = group),
    color = "blue") +
  scale_x_continuous("stimulus", breaks = -3:3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()
ggsave(file = "../figures/simulation-example-psychometric-fit-noLapse.png", width = 5, height = 3.5)
```

Next, we add the ground truth functional relation, including the lapse rate:

```{r simulation-example-data-plots-add-ground-truth, warning=FALSE}
p = d.temp %>%
  ggplot(aes(x = stimulus, y = accuracy)) +
  geom_hline(yintercept = 1- true.lambda, linetype = 2, color = "darkgray") +
  geom_hline(yintercept = true.gamma, linetype = 2, color = "darkgray") +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange") +
  geom_line(
    data = tibble(stimulus = seq(0, 3, length.out = 100), group = 1),
    aes(
      y = inv_logit_scaled(
        true.intercept + true.slope * stimulus, 
        lb = true.gamma, 
        ub = 1 - true.lambda),
      group = group),
    color = "darkgray", alpha = .5) +
  annotate(geom = "text", 
           label = expression(gamma + (1 - gamma - lambda)*~logit^{-1}~(alpha + beta*~stimulus)), 
           x = 3, y = 0, hjust = 1, color = "darkgray", fontface = "bold", size = 4) +
  annotate(geom = "text", label = substitute(paste(lambda, "=", true.lambda), list(true.lambda = true.lambda)), 
           x = 3, y = 1 - true.lambda - .05, hjust = 1, color = "darkgray", size = 4) +
  annotate(geom = "text", label = substitute(paste(gamma, "=", true.gamma), list(true.gamma = true.gamma)), 
           x = 3, y = true.gamma - .05, hjust = 1, color = "darkgray", size = 4) +
  scale_x_continuous("stimulus", breaks = -3:3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw()
ggsave(p, file = "../figures/simulation-example-just-ground-truth.png", width = 5, height = 3.5)


p +
  geom_line(
    data = tibble(stimulus = seq(0, 3, length.out = 100), group = 1),
    aes(
      y = inv_logit_scaled(
        true.intercept + true.slope * stimulus, 
        lb = true.gamma, 
        ub = 1),
      group = group),
    color = "blue")
ggsave(file = "../figures/simulation-example-psychometric-fit.png", width = 5, height = 3.5)
```

And, finally, we can compare the *perceptual model*---i.e., the functional relation (intercept and slope) between the stimulus and the responses when the participant responds based on the stimulus---when the $\lambda$ is assumed to be 0 (blue) compared to the ground truth of $\lambda = .05$ (gray).

```{r simulation-example-data-plots-perceptual-model-only, warning=FALSE}
estimates = 
  update(analyses[[1]], newdata = d.temp, chains = 4, cores = 4, backend = "cmdstanr") %>% 
  fixef() %>% 
  as_tibble() %>% 
  pull(Estimate)

tibble(stimulus = seq(0, 3, length.out = 100)) %>% 
 ggplot(aes(x = stimulus)) +
  geom_line(
    aes(
      y = inv_logit_scaled(
        true.intercept + true.slope * stimulus)),
    color = "darkgray") +
  geom_line(
    aes(
      y = inv_logit_scaled(
        estimates[1] + estimates[2] * stimulus)),
    color = "blue") +
  annotate(geom = "text", label = "ground truth", x = 1.5, y = .75, hjust = 1, color = "darkgray", size = 4) +
  annotate(geom = "text", label = "estimate under\nassumption of 0 lapses", x = 1.5, y = .4, hjust = 0, color = "blue", size = 4) +
  scale_x_continuous("stimulus", breaks = -3:3) +
  scale_y_continuous("accuracy\n(when responding based on stimulus)", limits = c(0,1)) +
  theme_bw()
ggsave(file = "../figures/simulation-example-psychometric-fit-just-responses-based-on-stimulus.png", width = 5, height = 3.5)
```

For the simulation, we obtain 100 simulated data sets for each of a variety of combinations of values for the true intercept, slope, and lambda (gamma is always .5 since we're simulated a 2AFC task). e will consider four different analyses, each assuming a different fixed lapse rate (0, .025, .05, .1). 

```{r simulation-loop} 
if (file.exists("../models/simulation.RData")) {
  load(file = "../models/simulation.RData") 
} else {
  d.sim = tibble(.rows = 0)
  
  par.true.intercept = c(-1.5)
  par.true.slope = c(1.5)
  par.true.lambda = c(.05)
  par.true.gamma = c(.5)
  par.n_simulations = 100
  par.n_per_stimulus = c(50)
  par.stims = list(
    seq(0, 3, length.out = 5),
    seq(1, 2, length.out = 5),
    seq(1, 3, length.out = 5))
  
  # ground truth
  for (true.intercept in par.true.intercept) {
    for (true.slope in par.true.slope) {
      for (true.lambda in par.true.lambda) {
        for (true.gamma in par.true.gamma) {
          # sample sizes
          for (n in par.n_per_stimulus) {
            # stimulus regimes
            for (s in par.stims) {
              for (k in 1:par.n_simulations) {
                d.temp = generate_data(
                  intercept = true.intercept, slope = true.slope, gamma = true.gamma, lambda = true.lambda, 
                  stimuli = s, n_per_stimulus = n)
                
                # analysis assumptions
                for (i in 1:length(par.lambda)) {
                  for (j in 1:length(par.gamma)) {
                    print(paste("simulation:", k, "; analysis:", paste(par.lambda[i], par.gamma[j]), "; stimuli:", paste(s, collapse = ","), "; n per stim:", n))
                    print(paste("true intercept:", true.intercept, "; slope:", true.slope, "; lambda:", true.lambda, "; gamma:", true.gamma))
                    model <- update(
                      analyses[[paste(par.lambda[i], par.gamma[j])]],
                      newdata = d.temp,
                      cores = 4,
                      chains = 4,
                      refresh = 0, 
                      backend = "cmdstanr")
                    
                    d.sim %<>%
                      rbind(
                        model %>%
                          fixef %>%
                          as_tibble(rownames = "term") %>%
                          mutate(simulation = k,
                                 model.lambda = par.lambda[i], model.gamma = par.gamma[j],
                                 n_per_stimulus = n, stimuli = paste(s, collapse = ","),
                                 true.intercept = true.intercept, true.slope = true.slope, true.lambda = true.lambda, true.gamma = true.gamma,
                                 divergent = model %>% nuts_params() %>% filter(Parameter == "divergent__") %>% summarise(Value = sum(Value > 0)) %>% pull()))
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  d.sim %<>%
    distinct(simulation, model.lambda, model.gamma, n_per_stimulus, stimuli, 
             true.intercept = true.intercept, true.slope = true.slope, true.lambda = true.lambda, true.gamma = true.gamma, term, .keep_all = T) %>%
    select(simulation, model.lambda, model.gamma, n_per_stimulus, stimuli, 
           true.intercept = true.intercept, true.slope = true.slope, true.lambda = true.lambda, true.gamma = true.gamma,
           term, everything()) %>%
    mutate(term = plyr::mapvalues(term, c("eta_Intercept", "eta_stimulus"), c("intercept", "slope")))
  save(d.sim, file = "../models/simulation.RData")
}
```

### Visualizing the correlation among the intercept and slope

The following plots illustrate the correlation among the intercept and slope in the different simulation conditions.

```{r simulation-results-plots-correlations}
p = d.sim %>%
  filter(true.intercept == -4, true.slope == 3) %>%
  pivot_wider(
    values_from = c(Estimate, Est.Error, Q2.5, Q97.5),
    names_from = term) %>%
  ggplot(aes(x = Estimate_intercept, y = Estimate_slope, color = stimuli)) +
  geom_point(alpha = .05, size = 1) +
  geom_point(
    data = . %>%
      group_by(true.intercept, true.slope, stimuli) %>%
      summarise_at(vars(Estimate_intercept, Estimate_slope), mean)) +
  scale_x_continuous("intercept estimate") +
  scale_y_continuous("slope estimate") +
  scale_color_discrete(
    "stimulus distribution\nacross performance",
    breaks = levels(factor(d.sim$stimuli)),
    labels = c("low+mid+high performance", "mid performance", "mid+high performance")) +
  theme_bw()
ggsave(p, file = "../figures/simulation-correlation-intercept-slope-alpha-4-slope3.png", height = 4, width = 6)

p %+%
  (d.sim %>% 
     pivot_wider(
    values_from = c(Estimate, Est.Error, Q2.5, Q97.5),
    names_from = term)) +
  geom_hline(
    data = 
      crossing(
        true.intercept = unique(d.sim$true.intercept),
        true.slope = unique(d.sim$true.slope)) %>%
      mutate(truth = true.slope),
    aes(yintercept = truth),
    linetype = 2, color = "darkgray") +
  geom_vline(
    data = 
      crossing(
        true.intercept = unique(d.sim$true.intercept),
        true.slope = unique(d.sim$true.slope)) %>%
      mutate(truth = true.intercept),
    aes(xintercept = truth),
    linetype = 2, color = "darkgray") +
  facet_grid(true.slope ~ true.intercept, labeller = "label_both")
ggsave("../figures/simulation-correlation-intercept-slope.png", height = 8, width = 9)
```

### Visualizing bias in the estimates

In the following plots, the gray dashed lines indicate the ground truth. We are summarizing the distribution of estimates for the intercept and slope obtained for the different types of simulation runs (each with 100 simulation samples).

```{r plotting-simulation-plots-results}
p = d.sim %>%
  filter(
    true.slope == 3,
    stimuli == levels(factor(d.sim$stimuli))[1]) %>%
  group_by(model.lambda, model.gamma, n_per_stimulus, stimuli) %>%
  ggplot(aes(x = model.lambda, y = Estimate, color = stimuli)) +
  geom_vline(xintercept = true.lambda, linetype = 2, color = "darkgray") +
  geom_hline(
    data = 
      crossing(
        term = c("intercept", "slope"),
        true.intercept = unique(d.sim$true.intercept)) %>%
      mutate(truth = ifelse(term == "intercept", true.intercept, true.slope)),
    aes(yintercept = truth),
    linetype = 2, color = "darkgray") +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = position_dodge(.01)) +
  scale_x_continuous("lambda assumed in analysis\n(gamma is set to ground truth chance)") +
  scale_color_discrete(
    "stimulus distribution\nacross performance",
    breaks = levels(factor(d.sim$stimuli)),
    labels = c("low+mid+high performance", "mid performance", "mid+high performance")) +
  facet_grid(term ~ true.intercept, scales = "free_y", labeller = "label_both") + 
  theme_bw()
ggsave(p, file = "../figures/simulation-bias-of-fixed-lambda-analyses-just-uniform-stimulus-scheme-true-slope3.png", height = 8, width = 9)

d.sim %>%
filter(
    true.slope == 3) %>%
  group_by(model.lambda, model.gamma, n_per_stimulus, stimuli) %>%
  ggplot(aes(x = model.lambda, y = Estimate, color = stimuli)) +
  geom_vline(xintercept = true.lambda, linetype = 2, color = "darkgray") +
  geom_hline(
    data = 
      crossing(
        term = c("intercept", "slope"),
        true.intercept = unique(d.sim$true.intercept)) %>%
      mutate(truth = ifelse(term == "intercept", true.intercept, true.slope)),
    aes(yintercept = truth),
    linetype = 2, color = "darkgray") +
#  geom_point(alpha = .05, size = 1, position = position_dodge(.01)) +
  stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = position_dodge(.01)) +
#  stat_summary(fun = mean, geom = "line", position = position_dodge(.01)) +
  scale_x_continuous("lambda assumed in analysis\n(gamma is set to ground truth chance)") +
  scale_color_discrete(
    "stimulus distribution\nacross performance",
    breaks = levels(factor(d.sim$stimuli)),
    labels = c("low+mid+high performance", "mid performance", "mid+high performance")) +
  facet_grid(term ~ true.intercept, scales = "free_y", labeller = "label_both") + 
  theme_bw()
ggsave("../figures/simulation-bias-of-fixed-lambda-analyses-true-slope3.png", height = 8, width = 9)
```


## Working with real data (unknown ground truth)

Next, we illustrate the consequences of assumption about $\lambda$ for data from an actual experiment. We again use Ashley's data, this time focusing on Subject 3 from Ashley. Unlike in the simple example shown above, we now consider the effect of an experimental condition (crowdedness). For a description of this condition, see the GLMM tutorial. Here, we test whether the crowdedness of the display affected performance and whether this effect interacted with the effect of stimulus size. We do so while comparing psychometric models that differ in their assumptions about $\lambda$.

For all models, we assume that $\gamma = .25$ (chance level accuracy in the 4AFC task). Like in the simulation study, all models considered in this section assume a logistic perceptual model. The models do, however, differ in their assumptions about $\lambda$. 

### Assuming a $\lambda$

The first model assumes $\lambda = 0$ (no lapses). This model seems to present some evidence for an interaction between the crowdedness condition and the effect of stimulus size:

```{r subject3-lapse0, echo=T}
BF <- brmsformula(
  ResponseCorrect ~ .25 + (1-.25-0) * inv_logit(eta),
  eta ~ 1 + Size * Condition,
  family = bernoulli(link="identity"),
  nl = TRUE) 
```

```{r, message=FALSE, warning=FALSE}
my.priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"))

fit.nl.s3.lambda0 <- brm(
  BF,
  data = d %>%
    filter(Subject == "3"),
  control = list(adapt_delta = 0.999),
  iter = 4000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject3-lambda0")

my_hypotheses(fit.nl.s3.lambda0)
p = plot(
  conditional_effects(
    fit.nl.s3.lambda0,
    conditions = make_conditions(d, "Condition")), 
  plot = F,
  points = T, 
  point_args = list(alpha = .05, height = .025, width = .025),
  theme = theme_fivethirtyeight())[[1]] 
p + p.common
```

The next two models assume non-zero lapse rates of $\lambda = .05$ and $\lambda = .2$, respectively. Note how the strength of the evidence for an interaction systematically becomes weaker as we increase the $\lambda$. Of course, we do not know the ground truth, but it is worth noting that Subject 3 does not seem to achieve performance close to 100% accuracy even for the largest stimuli tested (there are both accurate and inaccurate answers at the largest size values in the crowded condition). Indeed, we will see in the next section that the *estimated* $\lambda$ is close to 25%.

```{r subject3-lapse05}
BF <- brmsformula(
  ResponseCorrect ~ .25 + (1-.25-0.05) * inv_logit(eta),
  eta ~ 1 + Size * Condition,
  family = bernoulli(link="identity"),
  nl = TRUE) 
```

```{r, message=FALSE, warning=FALSE}
fit.nl.s3.lambda0.05 <- brm(
  BF,
  data = d %>%
    filter(Subject == "3"),
  control = list(adapt_delta = 0.999),
  iter = 4000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject3-lambda0.05")

my_hypotheses(fit.nl.s3.lambda0.05)
p = plot(
  conditional_effects(
    fit.nl.s3.lambda0.05,
    conditions = make_conditions(d, "Condition")), 
  plot = F,
  points = T, 
  point_args = list(alpha = .05, height = .025, width = .025),
  theme = theme_fivethirtyeight())[[1]] 
p + p.common
```



```{r subject3-lapse2}
BF <- brmsformula(
  ResponseCorrect ~ .25 + (1-.25-0.2) * inv_logit(eta),
  eta ~ 1 + Size * Condition,
  family = bernoulli(link="identity"),
  nl = TRUE) 
```

```{r, message=FALSE, warning=FALSE}
fit.nl.s3.lambda0.2 <- brm(
  BF,
  data = d %>%
    filter(Subject == "3"),
  control = list(adapt_delta = 0.999),
  iter = 4000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject3-lambda0.2")

my_hypotheses(fit.nl.s3.lambda0.2)
p = plot(
  conditional_effects(
    fit.nl.s3.lambda0.2,
    conditions = make_conditions(d, "Condition")), 
  plot = F,
  points = T, 
  point_args = list(alpha = .05, height = .025, width = .025),
  theme = theme_fivethirtyeight())[[1]] 
p + p.common
```

### Inferring $\lambda$ from the data 

We infer the lapse rate from the data while assuming that it is constant across conditions. This is the assumption made in most psychometric analyses. It is, however, worth noting that this assumption can introduce bias. We have seen this bias in the simulation study presented above (see also the readings listed at the beginning of this document). 

```{r subject3-lapse-inferred, echo=T}
BF <- brmsformula(
  ResponseCorrect ~ .25 + (1-.25-lambda) * inv_logit(eta),
  eta ~ 1 + Size * Condition,
  lambda ~ 1, 
  family = bernoulli(link="identity"),
  nl = TRUE) 
```

```{r, message=FALSE, warning=FALSE}
my.priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"),
  prior(beta(1, 1), nlpar = "lambda", lb = 0, ub = 1))

fit.nl.s3.lambda <- brm(
  BF,
  data = d %>%
    filter(Subject == "3"),
  control = list(adapt_delta = 0.999),
  iter = 4000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject3-lambda-inferred")

my_hypotheses(fit.nl.s3.lambda)
p = plot(
  conditional_effects(
    fit.nl.s3.lambda,
    conditions = make_conditions(d, "Condition")), 
  plot = F,
  points = T, 
  point_args = list(alpha = .05, height = .025, width = .025),
  theme = theme_fivethirtyeight())[[1]] 
p + p.common
```

The estimated $\lambda$ is close to 25%. Of course, we do not know the ground truth for this data. It is possible, for example, that Subject 3's performance would continue to increase had the subject been tested at even larger stimulus sizes. And this means that estimating the joint distribution of $\lambda$, $\alpha$, and $\beta$ (as we have done here) can introduce bias into the estimated of $\alpha$ and $\beta$, despite the fact that it is the model that arguably more closely reflects that generative process underlying the data. Further adding to researchers' conundrum is the fact that the adaptive procedure used in this, and many other, experiments makes it particularly hard to reliably estimate $\lambda$ (see e.g., Prins, 2013). And this in turn can affect how biased and reliable the estimates of the *perceptual model* are ($\alpha$ and $\beta$, also Prins, 2013).






# Combining the data from all subjects: mixed-effects psychometric models

One appeal of approaching psychometric data from the perspective of GLMMs is that we can apply the same random effect approach as in GLMMs for the lapse rate model (which then falls into the class of NLMMs). This allows us to combined trial-level repeated measures data from multiple subjects while maintained the assumption of (condition) independence of the observations---i.e., while avoiding the Type I error-inflation that would otherwise result if we combined data from multiple subjects.

The following is an example of the NLMM specification of the mixed-effects logistic model fit to Ashley's data. In addition to combining the data from all 10 subjects in Ashley's data, we also expand the model to include the effect of the crowdedness condition and its interaction with the effect of stimulus size. That is, we are directly assessing the effects of interest (crowdedness condition) on the perceptual model (the $\alpha$ and $\beta$) at a *trial-level*. In this mixed-effect model, the fixed effects for stimulus size, condition, and the interaction are the population-level estimates---i.e., our best bet about the conclusion we would be warranted to draw about a representative (random) subject drawn from the *population of subjects* (not just our sample of 10 subjects). For this example, we further relaxed the range of $\lambda$ to be unconstrained. $\lambda$ is also allowed to vary across subjects (but not condition) and we model the full covariance matrix of all subject-specific effects, i.e., the covariance between the subject-specific $\lambda$s, intercepts, effects of stimulus size, crowdedness condition, and their interaction:
 
```{r, echo=T}
BF.lambda.mixed <- brmsformula(
  # reparameterizing the model by fitting lapses in log-odds space (in order to aid convergence)
  # this means we need to convert the lapses back into proportion space below.
  ResponseCorrect ~ .25 + (1 - .25 - inv_logit(lambda)) * inv_logit(eta),
  eta ~ 1 + Size * Condition + (1 + Size * Condition | g | Subject),
  lambda ~ 1 + (1 | g | Subject), 
  family = bernoulli(link="identity"),
  nl = TRUE)
```

## Results

Note how in this model we do not obtain *any* evidence in favor of a negative interaction between the effect of size and crowdedness condition. If anything, there is weak (non-decisive) support for a *positive* interaction: a slight *in*crease in the effect of size in the uncrowded condition. This is the conclusion we obtained when combining the data from all subjects while accounting for the hierarchical (grouping) structure of the data.

```{r, message=F, warning=T, error=F}
# note that I am not constraining the values of lambda here. But that could be achieved with the 
# lower bound (lb) and upper bound (ub) arguments for the prior.
my.priors.lapse <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"),
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "lambda"), 
  # prior(beta(1, 1), nlpar = "lambda", lb = 0, ub = qlogis(.75)),
  prior(cauchy(0,2.5), class = "sd", nlpar = "eta"),
  prior(cauchy(0,2.5), class = "sd", nlpar = "lambda"))

fit.lambda.mixed <- brm(
  BF.lambda.mixed,
  data = d,
  init = 0,
  control = list(adapt_delta = 0.9995, max_treedepth = 15),
  iter = 10000,
  thin = 5,
  prior = my.priors.lapse,
  file = "../models/subjectALL-lambda-mixed-logit")

summary(fit.lambda.mixed)
my_hypotheses(fit.lambda.mixed, link_lambda = "logit")
```

## Obtaining the psychometric of all subjects in all conditions from a single model

We visualize the predictions for each of 100 random posterior samples from each subjects (each curve corresponds to one posterior sample). These predictions are shown as the 100 thin lines. Together, they provide the information that we could alternatively express in a confidence interval, as the posterior samples of the model joint represent the posterior probability distribution estimated by the model. The solid lines shows the predictions based on the *means* of the posterior samples. This line does not have to fall in the center of the other lines is that the three parameters of the model can be correlated (within and across participants). Taking the mean, rather than marginalizing over the entire posterior distribution, ignores those correlations (but is a *lot* faster to compute and thus used for the present purpose). The parameter summaries at the top left of each graph show those mean parameter values---i.e., the parameters that describe the solid line:

```{r, fig.width=8, fig.height=16, warning=F, message=F}
get_samples_from_nonlinear_model = function(
  model, 
  subject = levels(model$data$Subject), # or levels to be plotted
  condition = levels(model$data$Condition), # or levels to be plotted
  n.draws = 100
) {
  model %>% 
    spread_draws(
      b_lambda_Intercept,
      b_eta_Intercept,
      b_eta_Size,
      b_eta_ConditionCrowded.vs.Uncrowded,
      `b_eta_Size:ConditionCrowded.vs.Uncrowded`,
      r_Subject__eta[Subject, coef],
      r_Subject__lambda[Subject, coef2],
      ndraws = n.draws) %>%
    pivot_wider(
      names_from = coef,
      values_from = r_Subject__eta,
    ) %>%
    mutate(
      b_lambda = b_lambda_Intercept + r_Subject__lambda,
      b_Intercept = b_eta_Intercept + Intercept,
      b_Size = b_eta_Size + Size,
      b_Condition = b_eta_ConditionCrowded.vs.Uncrowded + ConditionCrowded.vs.Uncrowded,
      b_SizeCondition = `b_eta_Size:ConditionCrowded.vs.Uncrowded` + `Size:ConditionCrowded.vs.Uncrowded`
    ) %>%
    ungroup() %>%
    mutate_at("Subject", factor) %>%
    filter(Subject %in% subject) %>%
    tidyr::expand(
      nesting(
        .draw, 
        Subject,
        b_lambda,
        b_Intercept,
        b_Size,
        b_Condition,
        b_SizeCondition),
      Size = seq(-2.5, 2.5, .1),
      Condition = condition) %>%
    mutate(
      Condition.num = contrasts(model$data$Condition)[Condition,])
}

# not yet used
get_samples_from_model = function(...) {
  if (any(family(model) == "mixture"))
    return(get_samples_from_mixture_model(...)) else
      return(get_samples_from_nonlinear_model(...))
}

# Select a subject and show categorization based on posterior draws 
plot_psychometric = function(
  model, 
  subject = levels(model$data$Subject), # or levels to be plotted
  condition = levels(model$data$Condition), # or levels to be plotted
  animate = T,
  gamma = .25,
  n.draws = 100
) {
  subject = unique(subject)
  condition = unique(condition)    

  p <- 
    get_samples_from_nonlinear_model(
      model = model, 
      subject = subject,
      condition = condition,
      n.draws = n.draws) %>%
    ggplot(aes(x = Size, color = Subject, linetype = Condition)) +
    # individual lines
    geom_line(
      aes(
        y = inv_logit_scaled(
          b_Intercept + 
          b_Condition * Condition.num + 
          b_Size * Size + 
          b_SizeCondition * Condition.num, 
          lb = gamma,
          ub = 1 - plogis(b_lambda)),
        group = paste(Subject, Condition, .draw)),
      alpha = if (!animate) .2 else 1) +
    # by mean (solid) lines
    { if (!animate) 
      geom_line(
        data = . %>% 
          group_by(Subject, Size, Condition) %>%
          summarise(
            b_Intercept = mean(b_Intercept + b_Condition * Condition.num),
            b_Size = mean(b_Size + b_SizeCondition * Condition.num),
            b_lambda = mean(plogis(b_lambda))),
        aes(
          y = inv_logit_scaled(
            b_Intercept + b_Size * Size, 
            lb = gamma,
            ub = 1 - b_lambda),
          group = paste(Subject, Condition)),
        size = 1.5) } + 
    scale_linetype("Condition") +
    theme_fivethirtyeight() +
    p.common + ylim(0,1) +
    geom_text(
      data = . %>% 
        { if (!animate) {
          group_by(., Subject, Condition) %>%
            summarise(
              b_Intercept = mean(b_Intercept + b_Condition * Condition.num),
              b_Size = mean(b_Size + b_SizeCondition * Condition.num),
              b_lambda = mean(plogis(b_lambda))) 
        } else { 
          mutate(., 
                 b_Intercept = b_Intercept + b_Condition * Condition.num,
                 b_Size = b_Size + b_SizeCondition * Condition.num,
                 b_lambda = plogis(b_lambda)) }},
      x = -2.5, y = 1,
      hjust = 0,
      vjust = 1,
      family = "Helvetica",
      size = 4,
      color = "black",
      aes(
        group = paste(Subject, Condition),
        label = paste0(expression(eta), " = ", round(b_Intercept, 2), " + ", round(b_Size, 2), " * Size\n", 
                       expression(gamma), " = ", round(gamma, 2), "\n", 
                       expression(lambda), " = ", round(b_lambda, 2)))) +
        facet_grid(Subject ~ Condition) 
  
  if (animate) 
    p <- p + transition_states(.draw, transition_length = 1, state_length = 1)
  
  return(p)
}

p <- plot_psychometric(model = fit.lambda.mixed, animate = F) 
plot(p)
ggsave(
  plot = p,
  file = "../figures/lambda-logistic-GLMM-bySubjectCondition.png",
  width = 8, height = 20)
```

We can use animation to illustrate the hierarchical nature of the model. **Note that this animation will only work in Acrobat Reader but not in Preview or other PDF readers.** Press the play button to start the animation. Each animation step correspond to a single posterior sample---i.e., one hypothesis about the parameter values for all fixed and random effects in the model. Note how the different posterior samples (each animation step) 'move in sync'. That is because the subject-specific predictions include the population-level predictions that are common to all subjects. 

```{r, fig.width=8, fig.height=14, fig.show='animate', animation.hook='gifski', interval=1/10, warning=F, message=F}
plot_psychometric(model = fit.lambda.mixed) 
```

## Visualizing correlations between $\lambda$ and the parameters of the perceptual model
We can also visualize the joint posterior distribution of the model's fixed effect parameters. For example, the following plot visualizes the posterior distribution of the intercept (the average $\alpha$ across the two crowdedness conditions), the slope of stimulus size (the average $\beta$ across the two crowdedness conditions), and $\lambda$. Each point is a single posterior sampel from the model. This reveals a correlation between the three parameters. This is an interactive plot. If you see a message that your browser/device cannot display OpenGL, you might have to compile (knit) this document to HTML (rather than PDF) or go through the code incrementally and then execute just this code chunk. Similar plots could be obtained for any combination of parameters, including the subject-specific estimates of any parameter (\texttt{\url{shinystan::launch_shinystan()}}) provides an interactive HTML graphical user interface that lets you investigate any such correlations; you might know \texttt{R}'s Shiny apps from [webpages like this reporting COVID statistics](https://mappingmonroe.maps.arcgis.com/apps/opsdashboard/index.html#/217749730f174776a3896b3e8950e03b)).

```{r, message=F, warning=F}
fit.lambda.mixed %>% 
  spread_draws(
    b_eta_Intercept,
    b_eta_Size,
    b_lambda_Intercept) %>%
  plotly::plot_ly(
    x = ~b_eta_Intercept,
    y = ~b_eta_Size,
    z = ~b_lambda_Intercept,
    alpha = .5)
```


# Alternative perceptual models

## Weibull regression

The Weibull model is not a GLM, even if not combined with the lapse rate model. The Weibull has two parameters, the shape parameter $k$ and the scale parameter $\lambda$. Only if the shape parameter is known, is the remaining model a GLM (see \url{https://stats.stackexchange.com/questions/277466/is-weibull-distribution-a-exponential-family}).


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Weibull$}} {} {}; %
    \node[det, right=of distribution] (shape) {$k$} ; %
    \node[det, above=of distribution] (mu) {$\lambda_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$log^{-1} = exp$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome) } {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu, shape} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{only scale parameter ($\lambda$) is inferred}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{orange}{$Weibull$}} {} {}; %
    \node[latent, right=of distribution] (shape) {$k$} ; %
    \node[det, above=of distribution] (mu) {$\lambda_i$} ; %
    \factor[above=of mu] {link} {left: \textcolor{blue}{$log^{-1} = exp$}} {} {}; %
    \node[obs, above=of link] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (link) (X) (outcome) } {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu, shape} {distribution} ; %
    \edge {link} {mu} ; %
    \edge {X, beta} {link} ; %
  }
  \caption{both shape ($k$ and scale parameters ($\lambda$) are inferred}
  \end{subfigure}
  \caption{The Weibull regression is {\em not} a GLM, unless the shape parameter $k$ is fixed (a). The Weibull model with both its scale and shape parameter can thus be seen as an infinite set of GLMs.}
\end{figure}





# Appendix

## Inferring $\gamma$ 

The examples shown above assume that $\gamma$ is known. That is commonly the case. However, there are cases in which we want to fit both $\gamma$ and $\lambda$. 

### Mixture formulation

When inferring both $\gamma$ and $\lambda$, it is substantially more efficient (and often necessary) to switch to the mixture formulation of the mode, as it allows us to orthogonalize $\gamma$ and $\lambda$. This makes this formulation of the model for computationally efficient, and more likely to converge. The mixture specification is, however, only available if the distributional family of the perceptual model is defined over real numbers (like the lapse part of the model). This is not, for example, the case for the Weibull (and hence Gumbel) model, which is defined over counts (integers). 

```{r mixture-GLMM}
my.priors.lapse.mixture <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = mu2),
  prior(cauchy(0,2.5), class = "sd", dpar = mu2)
)

# formulated as a mixture model, the lapsing logistic is related to the Wiechman and Hill formulation of the lapse model, as follows:
# theta1 = lapsing + guessing
# mu1 = guessing
# theta2 = 1 - lapsing - guessing
# mu2 = perceptual model
#
# the linear predictor for theta is in log-odds. the softmax function is applied to the thetas to obtain a vector of probabilities.
# (see help on brmsformula)
# fit.lapse.mixed.mixture <- brm(
#   bf(ResponseCorrect ~ 1, 
#      mu1 ~ 0,
#      mu2 ~ 1 + Size * Condition + (1 + Size * Condition | Subject), 
#      theta1 ~ 1 + (1 | Subject)),
#   data = d, 
#   family = mixture(bernoulli("logit"), bernoulli("logit"), order = "none"),
#   init = 0,
#   control = list(adapt_delta = 0.999),
#   iter = 2000,
#   thin = 1,
#   prior = my.priors.lapse.mixture,
#   file = "../models/subject-lapse-mixed-mixture-repaired")

fit.lapse.mixed.mixture <- brm(
  bf(ResponseCorrect ~ 1, 
     mu1 ~ 1 + (1 | Subject),
     mu2 ~ 1 + Size * Condition + (1 + Size * Condition | Subject), 
     theta1 ~ 1 + (1 | Subject)),
  data = d, 
  family = mixture(bernoulli("logit"), bernoulli("logit"), order = "none"),
  init = 0,
  control = list(adapt_delta = 0.999),
  iter = 2000,
  thin = 1,
  prior = my.priors.lapse.mixture,
  file = "../models/subject-lapse-mixed-mixture-gamma-lambda")

my_hypotheses_mixture(fit.lapse.mixed.mixture)
```


```{r mixture-GLMM-functions}
get_gamma = function(theta, mu) {
  plogis(theta) * plogis(mu)
}

get_lambda = function(theta, mu) {
  plogis(theta) * (1 - plogis(mu))
}

get_samples_from_mixture_model = function(
  model, 
  subject = levels(model$data$Subject), # or levels to be plotted
  condition = levels(model$data$Condition), # or levels to be plotted
  n.draws = 100
) {
  model %>% 
    spread_draws(
      b_theta1_Intercept,
      b_mu1_Intercept,
      b_mu2_Intercept,
      b_mu2_Size,
      b_mu2_ConditionCrowded.vs.Uncrowded,
      `b_mu2_Size:ConditionCrowded.vs.Uncrowded`,
      r_Subject__theta1[Subject, coef2],
      r_Subject__mu1[Subject, coef3],
      r_Subject__mu2[Subject, coef],
      ndraws = n.draws) %>%
    pivot_wider(
      names_from = coef,
      values_from = r_Subject__mu2,
    ) %>%
    mutate(
      b_theta1_Intercept = b_theta1_Intercept + r_Subject__theta1,
      b_mu1_Intercept = b_mu1_Intercept + r_Subject__mu1,
      b_mu2_Intercept = b_mu2_Intercept + Intercept,
      b_mu2_Size = b_mu2_Size + Size,
      b_mu2_Condition = b_mu2_ConditionCrowded.vs.Uncrowded + ConditionCrowded.vs.Uncrowded,  # check whether this still runs. was + Size, but it think that was a typo
      b_mu2_SizeCondition = `b_mu2_Size:ConditionCrowded.vs.Uncrowded` + `Size:ConditionCrowded.vs.Uncrowded`   # check whether this still runs. was + Size
    ) %>%
    ungroup() %>%
    mutate_at("Subject", factor) %>%
    filter(Subject %in% subject) %>%
    tidyr::expand(
      nesting(
        .draw, 
        Subject,
        b_theta1_Intercept,
        b_mu1_Intercept,
        b_mu2_Intercept,
        b_mu2_Size,
        b_mu2_Condition,
        b_mu2_SizeCondition),
      Size = seq(-2.5, 2.5, .1),
      Condition = condition) %>%
    mutate(
      gamma = get_gamma(theta = b_theta1_Intercept, mu = b_mu1_Intercept),
      lambda = get_lambda(theta = b_theta1_Intercept, mu = b_mu1_Intercept),
      Condition.num = contrasts(model$data$Condition)[Condition,])
}
```

Next, we visualize the joint posterior distribution of the parameters of the model. First, the distribution of lapse rate (mixture 1 weight), bias (mixture 2 mean), and mean of psychometric model (mixture 2):

```{r mixture-GLMM-plots-sample-correlations, warning=FALSE}
get_samples_from_mixture_model(model = fit.lapse.mixed.mixture) %>%
  plotly::plot_ly(type = "scatter3d", mode = "markers") %>%
  plotly::add_trace(
    x = ~b_theta1_Intercept,
    y = ~b_mu1_Intercept,
    z = ~b_mu2_Intercept,
    opacity = .05,
    marker = list(opacity = .05))
```

The reason for why sampling $\gamma$ and/or $\lambda$ directly is difficult becomes apparent when one looks at the distribution of $\gamma$ and/or $\lambda$---most of the values for both parameters are very close to one of the bounds (0): 

```{r}
get_samples_from_mixture_model(model = fit.lapse.mixed.mixture) %>%
  plotly::plot_ly(type = "scatter3d", mode = "markers") %>%
  plotly::add_trace(
    x = ~gamma,
    y = ~lambda,
    z = ~b_mu2_Intercept,
    opacity = .05,
    marker = list(opacity = .05))
```

Lapse rate vs. mean and slope of psychometric model:

```{r}
get_samples_from_mixture_model(model = fit.lapse.mixed.mixture) %>%
  plotly::plot_ly(type = "scatter3d", mode = "markers") %>%
  plotly::add_trace(
    x = ~b_theta1_Intercept,
    y = ~b_mu2_Intercept,
    z = ~b_mu2_Size,
    opacity = .05,
    marker = list(opacity = .05))
```

Finally, we visualize the predictions for each of 100 random posterior samples from each subjects (each curve corresponds to one posterior sample). The solid lines shows the predictions based on the *means* of the posterior samples. This line does not have to fall in the center of the other lines is that the three parameters of the model can be correlated (within and across participants). Taking the mean, rather than marginalizing over the entire posterior distribution, ignores those correlations (but is a *lot* faster to compute and thus used for the present purpose). The parameter summaries at the top left of each graph show those mean parameter values---i.e., those are the parameters of the solid line:

```{r mixture-GLMM-results-plots, fig.width=8, fig.height=16, message=FALSE}
# Select a subject and show categorization based on posterior draws 
plot_psychometric = function(
  model, 
  subject = levels(model$data$Subject), # or levels to be plotted
  condition = levels(model$data$Condition), # or levels to be plotted
  animate = T,
  n.draws = 100
) {
  subject = unique(subject)
  condition = unique(condition)    

  p <- 
    get_samples_from_mixture_model(
      model = model, 
      subject = subject,
      condition = condition,
      n.draws = n.draws) %>%
    ggplot(aes(x = Size, color = Subject, linetype = Condition)) +
    # individual lines
    geom_line(
      aes(
        y = inv_logit_scaled(
          b_mu2_Intercept + 
            b_mu2_Condition * Condition.num + 
            b_mu2_Size * Size + 
            b_mu2_SizeCondition * Condition.num, 
            # lb = .25,
            # ub = 1 - plogis(b_theta1_Intercept)),
            lb = gamma,
            ub = 1 - lambda),
        group = paste(Subject, Condition, .draw)),
      alpha = if (!animate) .2 else 1) +
    # by mean (solid) lines
    { if (!animate) 
      geom_line(
        data = . %>% 
          group_by(Subject, Size, Condition) %>%
          summarise(
            b_mu2_Intercept = mean(b_mu2_Intercept + b_mu2_Condition * Condition.num),
            b_mu2_Size = mean(b_mu2_Size + b_mu2_SizeCondition * Condition.num),
            gamma = mean(get_gamma(theta = b_theta1_Intercept, mu = b_mu1_Intercept)),
            lambda = mean(get_lambda(theta = b_theta1_Intercept, mu = b_mu1_Intercept)),
            b_theta1_Intercept = mean(plogis(b_theta1_Intercept))),
        aes(
          y = inv_logit_scaled(
            b_mu2_Intercept + b_mu2_Size * Size, 
            # lb = .25,
            # ub = 1 - plogis(b_theta1_Intercept)
            lb = gamma,
            ub = 1 - lambda),
          group = paste(Subject, Condition)),
        size = 1.5) } + 
    scale_linetype("Condition") +
    theme_fivethirtyeight() +
    p.common + ylim(0,1) +
    geom_text(
      data = . %>% 
        { if (!animate) {
          group_by(., Subject, Condition) %>%
            summarise(
              b_mu2_Intercept = mean(b_mu2_Intercept + b_mu2_Condition * Condition.num),
              b_mu2_Size = mean(b_mu2_Size + b_mu2_SizeCondition * Condition.num),
              gamma = mean(get_gamma(theta = b_theta1_Intercept, mu = b_mu1_Intercept)),
              lambda = mean(get_lambda(theta = b_theta1_Intercept, mu = b_mu1_Intercept)),
              b_theta1_Intercept = plogis(mean(b_theta1_Intercept))) 
        } else { 
          mutate(., 
                 b_mu2_Intercept = b_mu2_Intercept + b_mu2_Condition * Condition.num,
                 b_mu2_Size = b_mu2_Size + b_mu2_SizeCondition * Condition.num,
                 b_theta1_Intercept = plogis(b_theta1_Intercept)) }},
      x = -2.5, y = 1,
      hjust = 0,
      vjust = 1,
      family = "Helvetica",
      size = 4,
      color = "black",
      aes(
        group = paste(Subject, Condition),
        label = paste0(expression(eta), " = ", round(b_mu2_Intercept, 2), " + ", round(b_mu2_Size, 2), " * Size\n", 
                       expression(gamma), " = ", round(gamma, 2), "\n", 
                       expression(lambda), " = ", round(lambda, 2)))) +
                       # expression(lapse), " = ", round(b_theta1_Intercept, 2)))) +
        facet_grid(Subject ~ Condition) 
  
  if (animate) 
    p <- p + transition_states(.draw, transition_length = 1, state_length = 1)
  
  return(p)
}

p <- plot_psychometric(model = fit.lapse.mixed.mixture, animate = F) 
plot(p)
ggsave(
  plot = p,
  file = "../figures/lapsing-logistic-mixture-GLMM-bySubjectCondition.png",
  width = 8, height = 20)
```

And as an animationt to illustrate the covariation in the psychometric functions across subjects:

```{r, fig.width=8, fig.height=14, message=FALSE, fig.show='animate', animation.hook='gifski', interval=1/10}
plot_psychometric(model = fit.lapse.mixed.mixture, animate = T) 
```




### NLM formulation

A less computationally efficient (or practical) way of inferring both $\gamma$ and $\lambda$ uses the NLM formulation used above when inferring only $\lambda$. This is illustrated here for a single subject (Subject 1). Note that this model did not converge and should not be trusted. More constraining priors and/or longer sampling might achieve convergence. This illustrates the challenges of inferring both $\lambda$ and $\gamma$, and highlights the advantage of the mixture formulation. 

```{r, message=F, warning=T, error=F}
BF <- brmsformula(
  ResponseCorrect ~ gamma + (1-gamma-lambda) * inv_logit(eta),
  eta ~ 1 + Size,
  gamma ~ 1,
  lambda ~ 1, 
  family = bernoulli(link="identity"),
  nl = TRUE)

my.priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"),
  prior(beta(1, 1), nlpar = "gamma", lb = 0, ub = 1),
  prior(beta(1, 1), nlpar = "lambda", lb = 0, ub = 1))

fit.1 <- brm(
  BF,
  data = d %>%
    filter(Subject == "1"),
  control = list(adapt_delta = 0.999),
  iter = 8000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject-lapse-bias")

summary(fit.1)
plot(fit.1)
p = plot(conditional_effects(fit.1), 
         plot = F,
         points = T, 
         point_args = list(alpha = .05, height = .025, width = .025),
         theme = theme_fivethirtyeight())[[1]] 

p + p.common
```

This model fit does not converge, and a look at the posterior samples suggests why. This model has the typical hallmarks of unidentifiability with two solutions emerging (symmetrical around 0 log-odds for the logistic part of the model; symmetrica around .5 for the guess and lapse rate, which are expressed on the scale of proportions).

We refit the model while constraining the guess and lapse rate to be less than .5, i.e., less than half of the trials are attentional lapses. This is a very weak assumption. 

```{r, echo=T}
my.priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", nlpar = "eta"),
  prior(beta(1, 1), nlpar = "lambda", lb = 0, ub = .5),
  prior(beta(1, 1), nlpar = "gamma", lb = 0, ub = .5)
)
```

Let's refit the model with these revised priors to the data from Subject 1. This model converges, shows no signs of multimodality in the posterior, and reveals an effect of letter size (the 95\% credible interval does not include 0). The predictions of the model also make sense (e.g., that it converges against chance at 25\%). It is, however, worth noting that there is ** *substantial* uncertainty about the lapse rate and bias terms**. This makes sense: although we have a lot of trials for each subject, we have very little information about the effect of size on the subject's responses since we're only testing at a few points along the size continuum. Additionally, psychometric designs tend to test mostly in the mid-performane part of the continuum. This is the case here, too: note that most of the data is in the middle of this subject's data. These common properties of psychometric designs makes it difficult to reliably distinguish between the effect of stimulus and effects of lapsing.

```{r, message=F, warning=F, error=F}
fit.2 <- brm(
  BF,
  data = d %>%
    filter(Subject == "1"),
    control = list(adapt_delta = 0.999),
  iter = 8000,
  thin = 5,
  prior = my.priors,
  file = "../models/subject-lapse-bias-constrained"
)

summary(fit.2)
plot(fit.2)
p = plot(conditional_effects(fit.2), 
         plot = F,
         points = T, 
         point_args = list(alpha = .05, height = .025, width = .025),
         theme = theme_fivethirtyeight())[[1]]
p + p.common
```







# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
