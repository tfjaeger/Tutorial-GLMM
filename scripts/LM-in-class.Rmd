---
title: "The Linear Model (LM)"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{animate}
  - \usepackage{booktabs}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{tikz}
  - \usepackage{url}
  - \usetikzlibrary{bayesnet}
output:
  tufte::tufte_handout: default
  fontsize: 10pt
urlcolor: blue
---

```{r set-options, include=F}
library(knitr)
opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=FALSE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 250),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
library(tidyverse) # gotta be tidy
library(magrittr)  # pipe!

library(tufte)     # Tufte-style handout formatting
library(gganimate) # animation of plots

library(broom)     # working with model output
library(ggthemes)  # nice themes for plotting
```

```{r constants, include=F}
chains = 4

options(
  width = 1000,
  mc.cores = min(chains, parallel::detectCores()))

theme_set(theme_bw())
```

# Readings and assignments in *preparation* of this class

In preparation for class, please **read *James et al. (2013)*, Chapter 3 up to---but not including---3.2 (12 pages)**. This text is quite intuitive and assumes comparatively little background. But if you find yourself getting lost, please stop and first read or watch one of the many more informal introductions to linear models (e.g., [this one](https://medium.com/analytics-vidhya/linear-regression-an-intuitive-approach-ded05ed39a87)). Then try to read James et al. (2013).^[James et al. (2013) *An introduction to statistical learning* is the an attempt---a wonderful attempt, full of effective visualization, I think---to make the content of one of the most influential books in machine learning (“Statistical learning” by Tibshirani and colleagues, the developers of bootstrap and other important non-parametric approaches) accessible to a non-expert audience.] **You should plan for 1-2h of reading and summarizing your questions in preparation for this week of the class.**

Optionally, those of you have interests beyond the basics of the LM, and find it intriguing to ponder about why we use the models we use, might enjoy reading p. 73-78 of McElreath’s (2019) *Statistical Rethinking$^2$*. It’s an amazing book, and these few pages accessibly and thought-provokingly take you through the question of why we often feel comfortable assuming normality in our analyses (as is done in the linear model).

Also optionally, you might find it helpful to read and *work through* this document. **Please note that this document is providing R code only.** Some of the steps described here might have less direct solutions in Matlab or Python, so it is recommended that you start early.


# About this document
This PDF is generated from an R markdown document (using the \texttt{tufte} package for styling). If you're interested in understanding the code that generated the examples presented below, you can [clone the repo](https://github.com/tfjaeger/Tutorial-GLMM/). Both the PDF and the R markdown that the PDF is generated from are in the scripts/ folder. You can go through the document line by line, or make changes and 'knit' a new PDF with the press of a button.


# The linear model (LM)
LMs (aka linear regression) can be thought of as an example of a much larger family of models that describe an *outcome* variable $y$ (aka the *dependent* variable) as the function $g$ of one or more *predictor* variables $x_1$, ..., $x_k$ (aka *independent* variables), $y = g(x_1, ..., x_k)$. In this sense, the LM is a---very constrained---*predictive model*. Specifically, the LM assumes that $y$ is a draw from a Normal distribution $\mathcal{N}$ with constant standard deviation $\sigma$ and a mean $\mu$ that is a linear combination of all the predictors included in the model:

\begin{align}
y & \sim & \mathcal{N}(\mu, \sigma) \\
\mu & = & \beta_1 x_1 + ... \beta_k x_k  \\
 & = & X\beta
\end{align}

Here, $y$ is a column vector of outcome observations of length $n$ (e.g., a column in your data table), $X$ is is called the $n$ x $k$ *model (or design) matrix* (e.g., in the simplest case, a number of $k$ columns in your data table) and $\beta$ is a vector of weight values of length $k$ (aka coefficients)---one for each of the $k$ predictors in the model. 

\begin{marginfigure}
  \centering
  \tikz{ %
    \node[obs] (outcome) {$y_i$} ; %
    \factor[above=of outcome] {distribution} {left: \textcolor{gray}{$\mathcal{N}$}} {} {}; %
    \node[latent, right=of distribution] (sigma) {$\sigma$} ; %
    \node[det, above=of distribution] (mu) {$\mu_i$} ; %
    \node[obs, above=of mu] (X) {$x_i$} ; %
    \node[latent, right=of X] (beta) {$\beta$} ; %
    % plates
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(mu) (distribution) (X) (outcome)} {$\forall i=1 \ldots N $}; %
    \edge {distribution} {outcome} ; %
    \edge {mu} {distribution} ; %
    \edge {sigma} {distribution} ; %
    \edge {X, beta} {mu} ; %
  }
  \caption{A graphical model visualization of the linear model. Shaded circles are observable variables. White circles are latent variables. These are the variables that constitute the degrees of freedom of the model, the paramters that are fit to the data. Diamonds shapes indicate variables that are determined by other variables. The index $i = 1, ..., N$ refers identifies the observations (usually, the rows) in your data. Note that $\sigma$, unlike $\mu_i$ is assuemd to be constant across observations. This is the assumption of the homogeneity of variance.}\label{fig:lm}

\end{marginfigure}

In the literature, LMs are often written in a number of different ways. For instance, we can say that our expectation for $y$, ${\rm E}[y] = X\beta$, or that $y$ is a linear combination plus some normally distributed residual (prediction) error $\epsilon$, $y = X\beta + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma)$. You will likely encounter all of these different formulations. You might also encounter formulations that describe each individual observation of $y$ rather than the entire vector of observations, e.g., ${\rm E}[y_i] = \beta_1x_{1,i} + ... + \beta_kx_{k,i}$. 

Finally, a common---but arbitrary---convention is to use a different symbol for one particular predictor, the *intercept* $\alpha$: 

\begin{align}
y & \sim & \mathcal{N}(\mu, \sigma) \\
\mu & = & \alpha + \beta_1 x_1 + ... \beta_k x_k \\
 & = & \alpha x_0 + \beta_1 x_1 + ... \beta_k x_k
\end{align}

Here, $x_0$ is assumed to be the constant 1, i.e., $x_0 = 1$. This convention singles out $\alpha$, which helps to realize that it has an intuitive interpretation: it is the predicted value of the outcome $y$ when all $x_1 = ... = x_k = 0$. 


## Use of the LM in our field(s)
While the LM can be used as a predictive model---i.e., to make predictions about unseen data---this is rarely done in our fields. Rather, we tend to fit an LM to our data, and then conduct statistical inferences about the *effects* of one or more of the model's predictors based on the best-fitting parameterization of that model. Best-fitting parameterization refers to the choice of $\beta$s (and the residual standard deviation $\sigma$, though we don't tend to be interested in this parameter) that make the LM deliver the best predictions about the observed values of $y$.

We'll learn later what "best" means here, and how those best-fitting parameters and their standard errors are obtained. For now, we'll assume that those are known, and focus on what they tell us about the effects of the different predictors that are included in the LM.


## Assumptions of the LM
The formulation of the LM in (1)-(3) above entails several assumptions:

 1. **Independence**. Each observation $y$ is assumed to be drawn independent of all other observations.^[This is a very common assumption, shared e.g., by $t$-tests, ANOVA, $\chi^2$-tests, and many non-parametric tests. ]
 2. **Normality**. $y$ is drawn from a Normal distribution $\mathcal{N}$, and thus is assumed to be able to take any real value from $-\infty$ to $+\infty$.
 3. **Homogeneity of variance**. While ${\rm E[y] = \mu}$ is assumed to vary with the predictors $X$, the residual standard deviation around that mean $\sigma$ is assumed to be independent of $X$, and thus constant and identical across all observations. 
 4. **Linearity**. $y$ is linear in the $\beta$s. That is, the *effects* of predictors are assumed to be linear.
 5. **Additivity**. The *effects* of predictors are assumed to be additive: ${\rm E}[y]$ is assumed to be the weighted *sum* of the predictors.^[Assumptions 4 and 5 often confuse learners. To understand them it is important to distinguish between **input variables** and the **predictors** we derive from them. For example, in the case study presented below, one of the input variables is $VOT$. But we could include both $VOT$ and $VOT^2$ as predictors in the model. This would model both linear and quadratic effects of VOT, each of which is assumed linearly add to the predicted outcome $y$. Similarly, we can include interactions of the input variable $VOT$ and some other predictor in the model, which will add their product as a predictor to the model. That is, we can very much use an LM to ask whether two variables have super- or sub-additive interactions. The additivity assumption just means that the effects of the interaction predictor are assumed to be added to the effects of the other predictors in the model.] 

In practice, many of these assumptions are violated by the data we analyze. Critically, not all of these violations are equally detrimental to the reliability of our conclusions. For example, in simple 2x2 factorial experiments in the cognitive and neurosciences, the linearity assumption is trivially given. Even for experiments with continuous predictors (aka covariates), the linearity assumption *might* not do much harm if the relation between the predictors and the outcome is monotonic and the effects are sufficiently large. 

Other assumptions, however, do tend to matter in practice. For example, violations of the assumption of independence---e.g., for repeated-measures data---tend to result in inflated Type I errors (an increased rate of spurious significances). Similarly, violations of the assumption of homogeneity (or its weaker forms like the assumption of homoscedasticity) can substantially inflate Type I error rates. We will return to these issues in later lectures.


# Geometric intuitions
I won't repeat the geometric interpretation of LMs here, as the James et al. readings does a great job setting that up. Generally speaking, an LM with $k$ predictors (plus the intercept) can be thought of as defining a hyperplane in a $k+1$ dimensional space, where the additional dimension is the outcome. For instance, for a single predictor, the LM defines a line that describes the predicted outcome as a function of the single predictor. For two predictors, the LM defines a plane in the 3D space defined by the two predictors and the outcome, etc. 

Often it is more intuitive, however, to plot the data in fewer dimensions. For example, an LM with a continuous predictor, a categorical predictor (factor), and their interaction, describes multiple lines (that might differ in slope). An LM with a quadratic polynomial of a single continuous input variable will have two continuous predictors derived from that input variable. But rather than to plot the predictions in 3D space, we can plot them in 2D space, showing the effects of both the linear and the quadratic transforms of the input variable along a single axis.


# General workflow
```{r}
# download.file(
#   url = "https://github.com/hlplab/constraints-on-distributional-learning-VOT/blob/main/data/experiment-results.csv",
#   destfile = "../data/Tan-Jaeger.csv")

d <- 
  read.csv("../data/Tan-Jaeger.csv") %>%
  # Remove catch trials
  filter(!is.na(Response.Voicing)) %>%
  # Some variable typing
  mutate(
    across(c(ParticipantID, Block, Condition.Exposure), factor),
    Response.t = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  rename(VOT = Item.VOT, Condition = Condition.Exposure) %>%
  # making response variable a little easier to understand for non-language scientists
  mutate(Response.isT = ifelse(Response.Voicing == "voiceless", T, F)) 
```

Here's the general workflow I'd recommend for an LM analysis, following adequate data checks, variable typing (e.g., making sure that variables that are factors indeed are factors,, etc.) and filtering (e.g., removal of practice and/or catch trials, outlier exclusion, etc.):

 + Get data into table format, with each observation of your data corresponding to one row of the table and separate columns for the outcome and each of the input variables.^[What counts as an observation will depend on both your hypothesis (e.g., whether it is meant to test a hypothesis within a participant or across participants) and the type of model you're using (e.g., an LM vs. a linear mixed-effects model).]

 + Decide how to code the factors in your data, so that each of the factor input variables can be translated into one ore more numerical predictors.^[In R, the \texttt{contrasts()} function and the family of \texttt{contr.*} functions are very helpful in setting up the contrasts for your factors, without having the manually change the column in the data frame. This approach means that the factors in the data table remain factors (allowing character values), avoiding the confusion that can result when factors are manually transformed into numeric predictors that are stored in the data table.] Decide whether to transform your continuous input variables, e.g., by centering and/or standardizing them.

 + Define the regression formula $y \sim 1 + x_1 + ... + x_k$, where 1 stands for the constant 1. Together with the data table and information about the factor coding (which is stored as an attribute to the factor column in the data.frame in R and in many similar programs), this formula contains all the information that is required for a function intended to fit the model to the data. 

   A call to the LM-fitting function typically first implicitly creates the model matrix $X$. **Looking at the model matrix corresponding to an LM or other model can be a very effective way to learn to understand formula syntax in whatever language you're using.** By default, this model matrix will contain the constant 1 as the first column, followed by one column per *predictor*. Note that this can be more columns than the number of input variables. For example, factor input variables with $k$ distinct levels will result in $k-1$ distinct predictors, and thus $k-1$ distinct columns in the model matrix. Similarly, interactions in the formula add additional columns to the model matrix (more on that in a later lecture). And there are a number of other common formula abbreviations that will expand into multiple columns in the design matrix. If you're not sure what a formula does, I recommend simply looking at the model matrix it creates. E.g., in R:

```{r model-matrix, echo=TRUE}
model.matrix(lm(data = d, formula = Response.RT ~ 1 + Condition * VOT)) %>%
  # Select 20 random rows from the model matrix for illustration
  as_tibble() %>%
  slice_sample(n = 20)
```

Note that the formula that we use to define our LM contains only all the *known* or *observable* variables, both the outcome variable and the input variables. The *unknown latent* variables---the $\beta$s and $\sigma$---are not mentioned in the formula. That's because those are the variables we're asking to be *estimated*, e.g., one $\beta$ for each predictor. The estimates that we get from fitting the LM are what is reported in the output of the LM if you print or summarize it.


# Working through an example
Next, we'll go through an example. We'll use the data from [Tan & Jaeger (2024). *Learning to understand an unfamiliar talker: Testing models of adaptive speech perception*](https://osf.io/5u8ew?view_only=270fc732415a49f5ab8f1fcaebf46b30). The data come from an experiment on adaptive speech perception, and contain both responses and reaction times from a two-alternative forced-choice task (2AFC). From the abstract of Tan & Jaeger:

```{r, fig.margin = TRUE, fig.cap = 'The same simple 2AFC task on all trials. Participants started each trial by clicking the green button. A recording played, and {\\bf participants had to answer which of the two words they heard}.', warning=FALSE, message=FALSE, cache=TRUE, out.width="80%"}
knitr::include_graphics("figures/trial_example.png")
```

>  [...] a few minutes of exposure can significantly reduce the processing difficulty listeners experience during initial encounters with an unfamiliar accent. How such adaptation unfolds incrementally, however, remains largely unknown, leaving basic predictions by theories of adaptive speech perception untested. [...] We begin to address these knowledge gaps in a novel incremental exposure-test paradigm. We expose US English listeners to shifted phonetic distributions of word-initial stops (e.g., "dill" vs. "till"), while incrementally assessing cumulative changes in listeners' perception. [...]

The following figure illustrates the design of the study, consisting of three between-participant exposure conditions, crossed with nine within-participant blocks, each eliciting responses along the phonetic VOT continuum (measured at 12 points). VOT refers to voice onset timing, and is the primary cue to the contrasts like *dip* and *tip* in US English. It is the time between the opening of a speaker's lips (release the air burst) and the start of vocal fold vibrations. As little as 5ms difference in that time can make the difference between hearing the word *dip* (lower VOT) and the word *tip* (higher VOT). The three between-participant condition differed only in the distribution of VOT during exposure blocks. 

```{r, fig.fullwidth = TRUE, fig.cap = 'Incremental exposure-test design. Histograms summarize distribution of voice onset time (VOT) in each block (dark bars: "d"-words; light: "t"-words). The three {\\em between-participant} conditions (rows) differed only in the distribution of VOT during exposure blocks. VOTs in test blocks were identical within \\& across conditions.', warning=FALSE, message=FALSE, cache=TRUE, out.width='100%'}
knitr::include_graphics("figures/block_design.png")
```

Specifically, the green condition has the exact same distribution as the red condition, but shifted 10ms to the right; the blue condition is shifted +40ms to the right. This design makes it possible to assess how participants' interpretation of the same physical VOT input during the gray test blocks changes as a function of the differently shifted VOT distributions during the exposure blocks. **This makes it possible to test whether listeners change their categorization functions over VOT as a function of recent input from a talker**.

121 participants were included for analysis, each providing 144 exposure trials (3 blocks) and 72 test trials (6 blocks). Participants were unaware of the exposure-test structure of the experiment, and the procedure for exposure and test trials was identical: participants saw two words on the screen (e.g., "dip" and "tip"). Participants then heard a recording of a single word and then had to click the word they heard. All recordings were spoken by the same voice. Here **we use only the data from the six gray test blocks**, leaving `r nrow(d)` observations. 
## The data
Since we want to avoid violations of the assumption of independence, we use the data from a single participant from the Shift40 condition, living us with $6 * 12 = 72$ observations. Here's some of the data of that participant. If you downloaded this R markdown document, feel free to change the participant, and see how that changes the results. 

```{r}
dd <- 
  d %>% 
  filter(
    Condition == "Shift40",
    Phase == "test") %>%
  filter(ParticipantID == first(ParticipantID)) %>%
  # select only the columns of the data that are relevant for this tutorial
  select(ParticipantID, Condition, Phase, Block, Trial, VOT, Response.isT, Response.RT) 

head(dd, 15)
```

## Example 1: Analyzing the effects of VOT on response times
We'll begin by analyzing the effects of VOT on response RTs. This makes response RTs our outcome variable, and VOTs the input variable. The first decision we need to make is how we'd like to code VOT. For example, we could treat VOT as a continuous predictor or as a factor with 12 distinct levels (the number of measurement locations along VOT). And, if we treat VOT as a continuous predictor, we could include only linear VOT or also quadratic VOT or higher order polynomials, provided we have enough data to avoid overfitting.^[As a rule of thumb, the number of coefficients in the LM should be less than 1/20th of the number of data points, $k < n / 20$. But that's just a rule of thumb, and details depend on how balanced the data is with regard to the distribution of predictor values.] Similarly, we could decide to include log-transformed VOT instead of VOT, or other transforms. As described above, the linearity assumption of LMs does not prevent us from applying non-linear transform to any of the input variables.

For the present example, we will treat VOT as a continuous predictor and only include its linear component.^[When we treat a predictor $x$ as continuous, we making the assumption that a change in its value of 1 unit has the same effect on the outcome $y$, regardless of where along $x$ that change is added. That follows from the linearity assumption mentioned above.]

```{r, echo=T}
m <- lm(formula = Response.RT ~ 1 + VOT, data = dd)
summary(m)
```

### Write-up
For this particular example, there isn't much to report. Note that we wouldn't usually case about the intercept, and rarely report it. So we might write:

\color{lightgray}
> To test our hypothesis, we used linear regression to regress the participant's reaction time against VOT. The effect of VOT was not significant ($p \geq .1$).

\color{black}
A few considerations:

 + Try to resist language that suggests significance even when it was not found, as in "... did not reach significance".
 + Do not take $p$-values to indicate effect strength. It is ok to write things like "highly significant effect" but that is not the same as a "large effect". Similarly, a "large effect"---in whatever way that is determined---is not necessarily the same a "theoretically particularly important" effect.

### Questions to ponder in class

 + How would you interpret the above output?
 + How would we expect the output to change if we centered VOT by subtracting the mean VOT from each VOT value before entering it into the model? Which estimates of the model would change? Which ones would stay the same?
 + How about if we also standardized VOT (by dividing it by its standard deviation)?
 + How could we test whether the slope of VOT changes across Blocks?
 
Bonus question:

 + How could we test a somewhat more interesting hypothesis that more ambiguous VOTs---VOTs that are more likely to elicit an equal proportion of "d"- and "t"-responses---are responded to more slowly? 

```{r, include=FALSE}
# One possible approach:

# determine ambiguity of VOT by fitting logistic regression to "d"/"t"-responses
# and get negative absolute log-odds as a measure of how ambiguous the token each 
# token is
dd$VOT_ambiguity <- -abs(predict(glm(Response.isT ~ 1 + poly(VOT, 2), data = dd)))

# Use that measure as a predictor of RTs
m <- lm(formula = Response.RT ~ 1 + VOT_ambiguity, data = dd)
summary(m)

# Alternatively, one could just hope that the ambiguous points are roughly in the 
# middle of the continuum, and test whether there is a negative quadratic effect 
# of VOT, but this approach makes far stronger assumptions that might not be (and 
# likely are not) justified.
m <- lm(formula = Response.RT ~ 1 + poly(VOT, 2), data = dd)
summary(m)
```

# Fitting
Consider the following fitted LM:

```{r}
# make a new numeric predictor nBlock 
dd %<>%
  mutate(nBlock = as.numeric(Block))

m <- lm(formula = Response.RT ~ 1 + nBlock, data = dd)
summary(m)
```

How did the model fitting algorithm arrive at the estimates and what do they present? What do the standard errors (SEs) of the estimates represent? Intuitively, we know that the estimates are the "best-fitting" estimates for the coefficients. What this means depends on how the goodness of fit is calculated. 

One way to find the best-fitting estimates for the coefficients of an LM is to minimize the total prediction error. For each observation in our data, the prediction error or *residual* of the model $m$ is defined as the difference between the predicted value of $y$, ${\rm E_m}[y]$, and the actually observed value of $y$, i.e., $residual = y - {\rm E_m}[y]$. Since we don't want negative and positive prediction errors (resulting from over- and under-predicting $y$, respectively) to cancel each other out, we cannot simply sum those prediction errors to get an estimate of how well the model fits our data. Instead, we sum the squared residuals, giving us the residual sum of squares (RSS).^[One could instead use any other function that would result in only positive prediction errors (e.g., taking the *absolute* prediction error). However, using the squared residuals has both appealing properties, and it turns out to be identical---for LMs---to maximizing the likelihood of the data.] Finding the best-fitting estimates then becomes the identical to finding the estimates that minimize the RSS.

To better understand what happens when we find the best-fitting estimates by minimizing the RSS, let's define a few function that help use deliver predictions just like the final LM presented above does, but for arbitrary intercept values $a$ and arbitrary slope values $b$ for the effect of Block. We also define a function that gets the prediction error for each observation in our data, and a function that sums up the squared prediction error:

```{r, echo=TRUE}
get_prediction <- function(a, b, data = dd) {
  a + b * as.numeric(dd$nBlock)
}

get_prediction_error <- function(a, b, data = dd) {
  dd$Response.RT - get_prediction(a, b, data = data)
}

get_RSS <- function(a, b, data = dd) {
  sum(get_prediction_error(a, b, data = data)^2)
}
```

Now we can use these functions to visualize the predictions and calculate the RSSs for different hypotheses about $a$ and $b$. You can use Acrobat Viewer to animate through the following examples (PDF system preview typically won't work):

```{r, , fig.show='animate', fig.cap="Raw RTs for one participants across the six test blocks (points), along with prediction line for different intercepts {\\em a} and slopes{\\em b} for the model \\texttt{RT} $\\sim$ \\texttt{1 + nBlock}. Compare how the RSS changes, depening on {\\em a} and {\\em b}."}
plot_prediction <- function(a, b, data = dd) {
  RSS <- get_RSS(a, b, data)
  
  data %>%
    ggplot(aes(x = nBlock, y = Response.RT)) +
    geom_point(alpha = .3) +
    geom_abline(aes(intercept = a, slope = b), color = "blue") +
    annotate(
      geom = "text",
      label = deparse(bquote("E[y]"== .(as.numeric(a))+.(as.numeric(b))*x)),
      parse = T,
      x = 9, y = 5000, hjust = 1, color = "blue", size = 6) +
    annotate(
      geom = "label", 
      label = deparse(bquote(RSS == .(RSS))), 
      parse = T,
      x = 9, y = 0, hjust = 1, vjust = 0, color = "orange", size = 6) +
    xlab("Block") + ylab("Response RT")
}

# plot prediction for best fit
plot_prediction(coef(m)[1], coef(m)[2])

# plot predictions for halved or doubled intercept
plot_prediction(coef(m)[1] / 2, coef(m)[2])
plot_prediction(coef(m)[1] * 2, coef(m)[2])

# plot predictions for zero or inverted slope
plot_prediction(coef(m)[1], coef(m)[2] * 0)
plot_prediction(coef(m)[1], coef(m)[2] * -1)
```

We can also visualize the RSS---i.e., the function we're trying to minimize---depending on the a wide range of values for $a$ and $b$. This illustrates two things. First, the estimates provided by the LM at the start of this section indeed minimize the RSS of the model. Second, RSS are a convex function of the parameters in the model (here $a$ and $b$), with a single optimum. This very appealing property of LMs allows for efficient fitting algorithms that are *guaranteed* to converge within an arbitrary finite number of steps against the optimal parameters. Basically, it means that going downhill is bound to get you closer to the optimum, rather than stuck in a local minimum (as can be the case for more complex optimization problems, e.g,. for many types of non-linear models).

```{r, fig.cap="RSS for the model \\texttt{RT} $\\sim$ \\texttt{1 + nBlock}, depending on its intercept $a$ and slope $b$. The white point shows that location of the estimates given in the LM output at the beginning of this section.", fig.height=4, fig.width=4, out.width="66%"}
# get RSS for a grid of a and b
d.rss <- 
  crossing(
    a = coef(m)[1] * seq(-10, 10, .1),
    b = coef(m)[2] * seq(-10, 10, .1)) %>%
  mutate(RSS = map2_dbl(a, b, ~ get_RSS(.x, .y, dd)))

d.rss %>%
  ggplot(aes(x = a, y = b)) +
  geom_tile(aes(fill = RSS)) +
  geom_point(color = "white", x = coef(m)[1], y = coef(m)[2]) +
  xlab("intercept a") + ylab("slope b") +
  scale_fill_viridis_b(trans = "log10") +
  coord_cartesian(expand = FALSE) +
  theme(legend.position = "top", legend.key.width=unit(1,"cm"))
```

Here's an example of how one can use a general optimizer to find the best-fitting estimates for an LM. The example uses R's \texttt{optim()} function but you can try to implement it in your preferred programming language.^[By default, \texttt{optim()} uses Quasi-Newtonian optimization. This works just fine for the present purpose.] You can compare its output to the estimates reported at the start of the section:

```{r, echo=T}
# let's store the optimizer's steps in a data.frame
d.trace <- tibble(.rows = 0)

o <- 
  optim(
    # Starting values for a and b
    par = c(0, 0), 
    fn = function(x) {
      rss <- get_RSS(x[1], x[2]);
      # Uncomment this line to see how optim() searches the parameter space
      # message(paste0("a = ", x[1], ", b = ", x[2], "; RSS = ", rss, "\n")) 
      # <<- assigns values to *global* variables (outside the function)
      d.trace <<- d.trace %>% bind_rows(tibble(a = x[1], b = x[2], RSS = rss))
      return(rss)
    })

# print optimal parameters found by optim()
print(o$par)
```

```{r, fig.margin=T, fig.height=4, fig.width=4, out.width="100%", fig.cap="Same as above but zoomed in and showing the path of the optimizer superimposed. Labels show step of optimizer", message=F, warning=F}
# add row numbers
d.trace %<>%
  mutate(step = as.numeric(row.names(.)))

last_plot() + 
  geom_path(
    data = d.trace, 
    color = "lightgray") +
  geom_label(
    data = d.trace %>% slice(c(1, 30, 60, 90, 120)),
    aes(label = step),
    color = "darkgray") +
  coord_cartesian(xlim = range(d.trace$a), ylim = range(d.trace$b))
```

### Apply what you learned
While minimization of RSS works well for LMs, there is a more generally applicable approach that yields the same results for LMs but also can be used for the fitting of many others types of models, *likelihood maximization*. Under this approach, we find the best-fitting estimates that maximize the likelihood of the observed outcomes $y$. Recall that the LM assumes $y$ to be normally distributed around the predicted ${\rm E}[y]$. This means we can calculate the *likelihood* of each observation under the hypothesized normal distribution. This normal distribution is a function of the hypothesized values for all the model's coefficients ($a$ and $b$ in the example above) *and* the value hypothesized for $sigma$. For any hypothesis about $a$, $b$, and $\sigma$, we can get the density of an observation $y$ under $\mathcal{N}(a + bx, \sigma)$. By multiplying the densities of all observations, we can get the data likelihood under the hypothesized values for $a$, $b$, and $\sigma$. In practice, we instead sum the log-transformed likelihoods. This is numerically more stable, and the $x$ that maximizes $\log(f(x))$ is the same that maximizes $f(x)$.

 + Define a function that calculates the sum of log-densities for the model \texttt{RT} $\sim$ \texttt{1 + nBlock} given a set of arguments $a$, $b$, and $\sigma$.^[In R, \texttt{dnorm()} will give you the density of an observation or vector of observations $y$ under a Normal with the specified mean and standard deviations. Use the optional argument \texttt{log = TRUE} to get log densities.]
 
 + Make a figure like the one shown above for RSS. For this figure set $\sigma$ to the $\sigma$ from the LM at the start of this section.^[In R, use \texttt{sigma()} to extract $\sigma$ from an LM.] We don't expect the likelihood values to be identical to the RSS values---they are on completely different scales. But do the surfaces have similar shapes (curvatures)?
 

# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
