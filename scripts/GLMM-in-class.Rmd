---
title: "Generalized Linear Mixed/Multilevel Models (GLMMs)"
subtitle: "In-class materials"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
  - \usepackage{caption}
  - \usepackage{subcaption}
  - \usepackage{tikz}
  - \usepackage{url}
  - \usetikzlibrary{bayesnet}
output:
  pdf_document: 
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  fontsize: 10pt
---

```{r set-options, include=F}
library(knitr)
opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=TRUE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 260),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
library(tidyverse) # gotta be tidy
library(magrittr)  # pipe!

library(lme4)      # frequentist GLMMs
library(brms)      # bayesian GLMMs
library(broom.mixed)     # working with model output
library(tidybayes) # working with posterior samples of bayesian models

library(sjPlot)    # model plotting
library(ggthemes)  # nice themes
library(ggforce)   # for powerful ggplot2 extensions
```

```{r constants, include=F}
chains = 4

options(
  width = 1000,
  mc.cores = min(chains, parallel::detectCores()))

theme_set(theme_wsj())
```



# Overview
This document is intended as in-class material. Make sure to first work through the homework in preparation for this class. We now continue to work through the same data set as in the homework.

```{r load data, include=F}
d = read_csv("../data/data_ClarkCrowding_TrialLevel.csv") 
d %<>%
  droplevels() %>%
  rename(
    Condition = Crowded,
    Threshold.Subj = Threshold,
    DiffusionConstant.Subj = DiffusionConstant,
    Curvature.Subj = Curvature,
    Span.Subj = Span,
    Area.Subj = Area,
    Speed.Subj = Speed,
    Size = Size,
    Size.AvgPerformance = Performance,
    ResponseExpected = Answer,
    ResponseCorrect = Correct,
    Curvature = TrialCurvature,
    DiffusionConstant = individualDiffusionConstant,
    Speed = TrialSpeed,
    Span = TrialSpan) %>%
  mutate(Condition = factor(ifelse(Condition == 0, "uncrowded", "crowded"), 
                            levels = c("uncrowded", "crowded"))) %>%
  select(Subject, Condition, 
         Threshold.Subj, DiffusionConstant.Subj, Area.Subj, Span.Subj, Speed.Subj, 
         everything(), 
         DiffusionConstant, Curvature, Span, Speed) %>%
  mutate_at(c("Subject"), factor)

str(d)
```

# Going over homework

Here is a possible write-up based on the many great examples you provided in your homework. I highlighted a few parts that not everyone had quite right (minor typos are not italicized). 

\begin{quote}
Figure XXX shows the proportion of correct responses by number size and crowdedness condition. We analyzed *trial-level* data with mixed-effects logistic regression using the function \texttt{glmer} from the lme4 package (Bates et al., 2013) in R (R Core Team 2020). Correct responses (correct response = 1 and an incorrect response = 0) were regressed against letter size (centered), condition (deviation coded, where .5 = crowded and -.5 = uncrowded) and their interaction. Following standards of the field, we fit the maximal random effect structure required by the design, random by subject intercepts and slopes for condition, size, and their interaction. *The model converged but indicated a possible singular fit. Since additional results with reduced random effect structures confirmed the qualitative results of the model with the full random effect structure, we report the latter.* 

We found a statistically significant main effect of letter size ($\hat{\beta}$ = 1.847, $z = 10.2$, $p < .01$), such that larger letter sizes were associated with better performance. We also found a significant effect of crowdedness ondition ($\hat{\beta}$ = -.929, $z = -6.9$, $p < .01$) such that performance was reduced in the crowded, compared to the uncrowded, condition. The interaction between letter size and condition was not found to be significant ($\hat{\beta}$ = -.101, $z = -.5$, $p > .5$).
\end{quote}

Note also a few other changes that reflect my personal preference:

 * I try to keep the sentence structure parallel between the different effects (e.g., "We found ..." *or* "There was ..." but not both).
 * I try to keep the perspective on the results constant across effects (e.g. "increased/decreased performance" *or* "higher/lower proportion of correct answers")
 * I try to code condition in a way that the values for the condition predictor are positive for the 'treatment' and negative for the 'control'/'baseline'. Similarly, I report the results using language that compares the treatment against the control rather than the other way around.





# Testing a causal (mediation) hypothesis

Sometimes we---or our reviewers---are interesting in ruling out confounding effects of other variables, or we have a specific hypothesis about how precisely our manipulation comes to affect the outcome. For example, what drives the effect of crowding that we observe in Ashley's data? Consider the following two competing hypotheses about the causal chain:

 * H1, direct causation: visual crowding $\rightarrow$ reduced accuracy
 * H2, indirect causation: visual crowding $\rightarrow$ different eye-movements (information seeking) $\rightarrow$ reduced accuracy
 
Note that H1 does not rule out that visual crowding affects eye-movements. It also does not rule out that changes in eye-movements affect the accuracy of our visual decision-making. But it *does* state that visual crowding has effects on accuracy that are not reducible to change in eye-movements that may or may not be caused by the crowding. **How can we test whether Ashley's data provide information that distinguishes between H1 and H2?** It turns out that one approach to this question involves hypothesis-driven model comparisons. So let's use this question to get an initial introduction to model comparison that avoid the utter open-endedness of more exploratory model comparison (which we'll learn about in another class).


## Mediation analysis

To compare H1 and H2, we need to fit and compare a few models:

 * **Model 1 (nested)** with only Size * Condition
 * **Model 2 (nested)** with only Size * Eye-movement measure
 * **Model 3 (nesting)** with Size * (Condition + Eye-movement measure), which is the model that *nests* the both of the first two models.
 
**For GL*M*Ms, it is important to keep in mind that we're interested in assessing the role of the *fixed* effects. We therefore use the *same random effect structure across all of the three models*.** If we didn't, whatever differences we find in the models' fit might be driven by differences in their random effects, and we want to avoid that. Specifically, this means that we're using the full random effect structure that includes random slopes for both condition and eye-movements in all of the three models.

Here, we use the *diffusion constant* as a measure of the relevant eye-movement. The fixed effects of the three resulting models are summarized below. Note that the combined model (model 3) indeed contains a superset of the predictors of the two nested models:

```{r}
center = function(x) { return(x - mean(x)) }

# the three models only differ in their formula. They are fit against the same data using the same method.
if (file.exists("../models/GLMM-for-model-comparison-without-diffusion-constant.rds")) {
  m.condition =  readRDS("../models/GLMM-for-model-comparison-without-diffusion-constant.rds")
} else {
  m.condition = glmer(
    ResponseCorrect ~ 1 + center(Size) * Condition + 
      (1 + center(Size) * (Condition + center(DiffusionConstant)) | Subject),
    data = d,
    control = glmerControl(
      optimizer = c("bobyqa"),
      optCtrl = list(
        npt = 10,
        maxfun = 2e6)),
    family = binomial)
  saveRDS(m.condition, file = "../models/GLMM-for-model-comparison-without-diffusion-constant.rds")
}

if (file.exists("../models/GLMM-for-model-comparison-without-condition.rds")) {
  m.DiffusionConstant = readRDS("../models/GLMM-for-model-comparison-without-condition.rds")
} else {
  m.DiffusionConstant = glmer(
    ResponseCorrect ~ 1 + center(Size) * center(DiffusionConstant) + 
      (1 +  center(Size) * (Condition + center(DiffusionConstant)) | Subject),
    data = d,
    control = glmerControl(
      optimizer = c("bobyqa"),
      optCtrl = list(
        npt = 10,
        maxfun = 2e6)),
    family = binomial)
  saveRDS(m.DiffusionConstant, file = "../models/GLMM-for-model-comparison-without-condition.rds")
}

if (file.exists("../models/GLMM-for-model-comparison-both.rds")) {
  m.both = readRDS("../models/GLMM-for-model-comparison-both.rds")
} else {
  m.both = glmer(
    ResponseCorrect ~ 1 + center(Size) * (Condition + center(DiffusionConstant)) + 
      (1 +  center(Size) * (Condition + center(DiffusionConstant)) | Subject),
    data = d,
    control = glmerControl(
      optimizer = c("bobyqa"),
      optCtrl = list(
        npt = 10,
        maxfun = 2e6)),
    family = binomial)
  saveRDS(m.both, file = "../models/GLMM-for-model-comparison-both.rds")
}

# Only printing the fixed effects
tidy(m.condition) %>% filter(effect == "fixed") %>% select(-c(effect, group))
tidy(m.DiffusionConstant) %>% filter(effect == "fixed") %>% select(-c(effect, group))
tidy(m.both) %>% filter(effect == "fixed") %>% select(-c(effect, group))
```
 
Specifically, we will compare the third against the first model, and against the second model. If the nesting model is better than both of the nested models, then both crowding and eye-movement make unique contributions to explaining variability in the outcome. If the nesting model is better than the first model, but cannot be distinguished from the second model then the effect of crowding---which we already know to exist---is entirely subsumed by the effect of eye-movements. That is, eye-movements would explain all variability in the outcome that crowdedness explains, and more. If the nesting model is better than the second model, but not better than the first model then crowding explains any potential effect of eye-movements on the outcome (in order to determine whether the eye-movements have any effect at all, we'd have to look at the second model). Finally, if the fit of the nesting model is indistinguishable from either of the two nested models then the effect of crowding and eye-movements cannot be distinguished between by this data.

So, how do we determine which models are 'better'?


## Model comparison

Both of the model comparisons (model 3 against model 1, and model 3 against model 2) are *nested model comparisons* since model 3 contains all predictors contained in model 1 or model 2. As a measure of the goodness of fit, we can use the model's deviance (recall, this is simply the model's log likelihood * -2). As the name suggests, deviance is a measure of error, so smaller is better and larger is worse. When we add predictors to a model the maximum likelhood fit of the new model will always provide at least as good or better a fit against the data as the original model. That is, deviance will never go up when we add a predictor to a maximum likelihood-fitted GLMM. This makes sense: if we provide the model with more information that it can use, it will perform at least as well as before (since we're only ever considered the model parameterization that maximize the probability of the data under the model). Let's call the difference between the nested model's deviance minus the nesting model's deviance $\delta_{deviance}. This difference will always be 0 or larger.

But we don't want to just say that models with less error are automatically *significantly* better. To determine whether a change in the deviance is significant, we compare it against the change in the complexity of the model. We measure this additional complexity as the increase in the number of degrees of freedom from the nested to the nesting model. Let's call this difference in the number parameters in the model $\delta_k$. In the limit, the difference in deviance between the two models approximates a $\chi^2$-distribution with $\delta_k$ degrees of freedom. That is, if we have sufficiently much data we can assess whether a model fit has improved significantly by asking whether $\chi^2(\delta_k) = \delta_{deviance}$ is significant. 

In R and Matlab, the anova functions allow us to directly compare nested LMs, GLMs, or GLMMs. For example, for the comparison of the nesting model (model 3) against the model without DiffusionConstant (model 1):

```{r, echo=T}
anova(m.both, m.condition, test = "chisq")
```

And for the comparison of the nesting model (model 3) against the model without condition (model 2):

```{r, echo=T}
anova(m.both, m.DiffusionConstant, test = "chisq")
```

### Discuss in class

 1. In your own words, what would you conclude from this mediation analysis?
 1. Looking back at the model without condition (model 2), notice how the effect of number size is reduced compared to the model without the diffusion constant (model 1). What do you make out of that? 

### Further readings on model comparison

Gelman and Hill (2007, Ch. 22) provides a quick introduction to the relation between ANOVA and model comparison. James et al. (2013) cover model selection in Ch. 6.1, under the heading "Subset selection". And, Harrell (2001) provides an excellent introduction to the general principles of model selection on the first 70ish pages of his book.






# Bayesian GLMMs

Data analyses standards keep changing, and it seems fair to say that there is an increasing push towards Bayesian approaches. We cover the general pros and cons of Bayesian approaches in another class. Here I'll just recap a few main points of why someone might switch to Bayesian GLMMs for data analysis now:

 * Unlike $p$-values, Bayes Factors provide a coherent measure of the amount of evidence for/against a hypothesis (e.g., Wagenmakers, 2007).
   * This includes coherent measures for replication success that avoid the dichotomy of significance-based reasoning (e.g., Verhagen \& Wagenmakers, 2014; for a Bayesian replication test for GLMMs, see Xie et al., 2020).
 * Priors can help convergence, especially for more complex models. There are pros and cons to the role of priors, but weakly regularizing priors can serve as an intuitive implementation of Occam's razor without biasing the direction of the inferred effects. 
 * Access to the full posterior distribution of *all* parameters in the model. For GLMMs, this includes the estimated variances and correlations of the grouping (random) effects, which are point estimates under the frequentist approach. 

## An example: The Bayesian equivalent of the GLMM in the homework document

We can refit the same model we fit for the homework within a Bayesian GLMM. We first define weakly regularizing priors for the fixed effects, the variances of the random effects, and the correlations of these random effects following [recommendations for GLMM priors](https://statmodeling.stat.columbia.edu/2017/04/28/prior-choice-recommendations-wiki/):

```{r, echo=T}
my.priors <- c(
  prior(student_t(3, 0, 2.5), class = "b"),
  prior(cauchy(0,2.5), class = "sd"),
  prior(lkj(1), class = "cor")
)
```

Here, class "b" is the prior for the fixed effect coefficients (i.e., the **b**etas); class "sd" and "cor" are the priors for the **s**tandard **d**eviations and **cor**relations of the random effects, respectively. The prior for the fixed effects parameters, for example, is centered around 0 and attributes increasingly less probability mass to increasingly more extreme coefficient values. For these priors to make sense, it is important that they are on the right scale. The priors shown above are the recommendations for categorical predictors that are coded with 1 unit distance between levels and continuous predictors that have been scaled by *twice* their standard deviation (Gelman et al., 2008). We thus transform our predictors following accordingly:^[The same recommendations are also often given for frequentist models because they can aid convergence (by avoiding very large or small coefficients, increasing numerical accuracy during the model fitting) and facilitate comparison of effect sizes across predictors and studies (see Gelman et al., 2008).]

```{r, echo=T}
# We store the mean and sd of Size so that we can later transform the model's predictions back 
# onto the scale of original Size predictor
Size.mu = mean(d$Size)
Size.sd = sd(d$Size)

# Standardize Size and make sure order of levels for Condition is as intended
d %<>%
    mutate(
      Condition = factor(Condition, levels = c("uncrowded", "crowded")),
      Size = (Size - mean(Size)) / (2 * sd(Size)))

# Sum-code condition
contrasts(d$Condition) = cbind("Crowded.vs.Uncrowded" = c(-.5,.5))
```

Now we are ready to fit the Bayesian GLMM. We will be using the \texttt{brms} library. In this library, the formula is the same as for the frequentist GLMM:

```{r, echo=T, fig.height=3, fig.width=4, fig.show="hold", out.width="33%"}
bm <- brm(
  formula = ResponseCorrect ~ 1 + Size * Condition + 
    (1 + Size * Condition | Subject),
  data = d,
  family = bernoulli("logit"),
  iter = 2000,
  prior = my.priors,
  file = "../models/GLMM"
)

summary(bm)
plot(conditional_effects(bm), ask = F)
```
We can use Bayesian hypothesis tests to obtain a measure of the support for/against two competing hypotheses. For example, to assess the support for the hypothesis that increasing letter size results in better performance (against the hypothesis of the *opposite* effect, including a zero effect):


```{r, echo=T}
hypothesis(bm, "Size > 0")
```

Here, the evidence ratio is the Bayes factor in support of the hypothesis ($BF_{H_{\beta_{size}>0},H_{\beta_{size}<0}}$). The posterior probability is the posterior probability of the hypothesis being true given the data, the modeling assumptions we made, and a uniform prior over both hypotheses. The fact that the evidence ratio is reported as "infinite" just means that none of the 2000 posterior samples we obtained supported the competing hypothesis. So we might more cautiously write $BF_{H_{\beta_{size}>0},H_{\beta_{size}<0}} > 1999, p_{posterior}>.9995$. Following Rafterty (1995), Bayes Factors are sometimes described as providing "non-decisive" or "weak" support (BF 1-3), "positive" support (BF > 3), "strong"  support (BF > 20), or "very strong" support (BF > 150), though one might criticize this as running counter to the idea of avoiding dichotomizing language when summarizing results. In this case, we would conclude that we have "very strong" support for the hypothesis that performance increases with letter size, and---for the benefit of readers unfamiliar with this language---we might add that the support was "well above the limits conventionally required for significance" though such language would hopefully become unnecessary as more researchers become familiar with Bayesian data analysis.

And so forth for the other effects. Just like we might report the fixed effects of a frequentist analysis, we might report a table of hypotheses tests for the Bayesian analysis (there are many different ways of summarizing the posterior distribution of parameters, and so you will find many complementary recommendations in the literature; what I describe here is one acceptable approach).


```{r, echo=T}
hypothesis(bm, "ConditionCrowded.vs.Uncrowded < 0")
hypothesis(bm, "Size:ConditionCrowded.vs.Uncrowded < 0")
```

### Discuss in class

 1. How does the Bayesian model compare to the frequentist fit? What do they share and where do they differ? For comparison, here is the frequentist GLMM from the homework refit for the standardized size predictor:

```{r, echo=F, warning=F}
m = glmer(
  ResponseCorrect ~ 1 + Size * Condition + 
    (1 + Size * Condition | Subject),
  data = d,
  control = glmerControl(
    optimizer = c("bobyqa"),
    optCtrl = list(
      npt = 10,
      maxfun = 2e6)),
  family = binomial)

summary(m)
```

 2. We can visualize the correlation between the posteriors samples of different fixed effect parameters. What do you make out of these correlations? 

```{r, echo=F}
bm %>%
  spread_draws(
    b_Intercept,
    b_Size,
    b_ConditionCrowded.vs.Uncrowded,
    `b_Size:ConditionCrowded.vs.Uncrowded`
  ) %>%
  select(-c(.chain, .iteration, .draw)) %>%
  ggplot(
    aes(x = .panel_x, y = .panel_y)) + 
  geom_point(alpha = 0.2, shape = 16, size = 0.5) + 
  geom_autodensity() +
  geom_density2d() +
  geom_smooth(method = "lm", color = "red", size = .5) +
  facet_matrix(vars(everything()), layer.diag = 2, layer.upper = c(3, 4), 
               grid.y.diag = FALSE) + 
  theme_bw()
```


# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
